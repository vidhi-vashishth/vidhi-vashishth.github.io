{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multinomial Logit Model\"\n",
        "author: \"Vidhi Vashishth\"\n",
        "date: today\n",
        "---\n",
        "\n",
        "\n",
        "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data.\n",
        "\n",
        "(I deleted the code and loaded the code in Python.)\n",
        "\n",
        "\n",
        "\n",
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables."
      ],
      "id": "bc8d660f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data-prep-python\n",
        "#| echo: true\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Examine the structure of the conjoint data\n",
        "conjoint_data = pd.read_csv(\"conjoint_data.csv\")\n",
        "print(conjoint_data.head())\n",
        "\n",
        "# Create dummy variables for brands (using Hulu as reference level)\n",
        "conjoint_data['netflix'] = np.where(conjoint_data['brand'] == 'N', 1, 0)\n",
        "conjoint_data['prime'] = np.where(conjoint_data['brand'] == 'P', 1, 0)\n",
        "\n",
        "# Create dummy variable for ads (No ads is reference)\n",
        "conjoint_data['ads'] = np.where(conjoint_data['ad'] == 'Yes', 1, 0)\n",
        "\n",
        "# Organize data in format suitable for MNL estimation\n",
        "# Number of respondents, tasks, and alternatives per task\n",
        "n_resp = len(conjoint_data['resp'].unique())\n",
        "n_tasks = len(conjoint_data['task'].unique())\n",
        "n_obs = n_resp * n_tasks\n",
        "n_alts = 3\n",
        "\n",
        "# Initialize arrays to store the data\n",
        "choices = np.zeros((n_obs, n_alts))\n",
        "X_netflix = np.zeros((n_obs, n_alts))\n",
        "X_prime = np.zeros((n_obs, n_alts))\n",
        "X_ads = np.zeros((n_obs, n_alts))\n",
        "X_price = np.zeros((n_obs, n_alts))\n",
        "\n",
        "# Populate the arrays\n",
        "row_idx = 0\n",
        "for r in conjoint_data['resp'].unique():\n",
        "    for t in range(1, n_tasks + 1):\n",
        "        row_idx += 1\n",
        "        \n",
        "        # Get the data for this respondent and task\n",
        "        task_data = conjoint_data[(conjoint_data['resp'] == r) & (conjoint_data['task'] == t)]\n",
        "        \n",
        "        if len(task_data) == n_alts:\n",
        "            # Record the choice\n",
        "            choices[row_idx-1, :] = task_data['choice'].values\n",
        "            \n",
        "            # Record the covariates\n",
        "            X_netflix[row_idx-1, :] = task_data['netflix'].values\n",
        "            X_prime[row_idx-1, :] = task_data['prime'].values\n",
        "            X_ads[row_idx-1, :] = task_data['ads'].values\n",
        "            X_price[row_idx-1, :] = task_data['price'].values\n",
        "\n",
        "# Combine into a dictionary for easier access\n",
        "mnl_data = {\n",
        "    'choices': choices,\n",
        "    'X_netflix': X_netflix,\n",
        "    'X_prime': X_prime,\n",
        "    'X_ads': X_ads,\n",
        "    'X_price': X_price,\n",
        "    'n_resp': n_resp,\n",
        "    'n_tasks': n_tasks,\n",
        "    'n_alts': n_alts,\n",
        "    'n_obs': n_obs\n",
        "}\n",
        "\n",
        "# Verify we have one choice per task\n",
        "print(f\"Proportion of valid choices: {np.sum(np.sum(choices, axis=1) == 1) / choices.shape[0]}\")"
      ],
      "id": "data-prep-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data has now been reshaped into a format suitable for MNL estimation. We've created binary indicator variables for Netflix, Prime (with Hulu as the reference brand), and ads (with ad-free as the reference). For each respondent-task combination, we have a row in our matrix format, with columns representing the different alternatives.\n",
        "\n",
        "\n",
        "## 4. Estimation via Maximum Likelihood\n",
        "\n",
        "First, I'll implement the log-likelihood function for the MNL model:"
      ],
      "id": "20b750d0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mnl-loglik-python\n",
        "#| echo: true\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from scipy import linalg\n",
        "import pandas as pd\n",
        "\n",
        "# Define the log-likelihood function for the MNL model\n",
        "def mnl_loglik(beta, data):\n",
        "    # Extract parameters\n",
        "    b_netflix = beta[0]\n",
        "    b_prime = beta[1]\n",
        "    b_ads = beta[2]\n",
        "    b_price = beta[3]\n",
        "    \n",
        "    # Extract data components\n",
        "    choices = data['choices']\n",
        "    X_netflix = data['X_netflix']\n",
        "    X_prime = data['X_prime']\n",
        "    X_ads = data['X_ads']\n",
        "    X_price = data['X_price']\n",
        "    n_obs = data['n_obs']\n",
        "    n_alts = data['n_alts']\n",
        "    \n",
        "    # Initialize log-likelihood\n",
        "    loglik = 0\n",
        "    \n",
        "    # Loop through each choice task\n",
        "    for i in range(n_obs):\n",
        "        # Calculate deterministic utility for each alternative\n",
        "        V = b_netflix * X_netflix[i, :] + \\\n",
        "            b_prime * X_prime[i, :] + \\\n",
        "            b_ads * X_ads[i, :] + \\\n",
        "            b_price * X_price[i, :]\n",
        "        \n",
        "        # Calculate choice probabilities\n",
        "        exp_V = np.exp(V)\n",
        "        probs = exp_V / np.sum(exp_V)\n",
        "        \n",
        "        # Find the chosen alternative and add its log probability to the log-likelihood\n",
        "        chosen = np.where(choices[i, :] == 1)[0]\n",
        "        if len(chosen) > 0:  # Ensure there is a chosen alternative\n",
        "            loglik += np.log(probs[chosen[0]])\n",
        "    \n",
        "    return loglik\n",
        "\n",
        "# Define the negative log-likelihood for optimization\n",
        "def neg_mnl_loglik(beta, data):\n",
        "    return -mnl_loglik(beta, data)"
      ],
      "id": "mnl-loglik-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, I'll use optim() to find the maximum likelihood estimates:"
      ],
      "id": "2bf7650d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mle-estimation-python\n",
        "#| echo: true\n",
        "\n",
        "# Initial parameter values\n",
        "beta_init = np.array([0, 0, 0, 0])\n",
        "\n",
        "# Run the optimization to find MLEs\n",
        "mle_results = minimize(neg_mnl_loglik, beta_init, args=(mnl_data,), \n",
        "                      method='BFGS', options={'disp': True})\n",
        "\n",
        "# Extract the parameter estimates\n",
        "beta_mle = mle_results.x\n",
        "param_names = [\"Netflix\", \"Prime\", \"Ads\", \"Price\"]\n",
        "\n",
        "# Calculate the Hessian at the optimum using numerical approximation\n",
        "def compute_hessian(f, x, eps=1e-5, *args):\n",
        "    n = len(x)\n",
        "    hessian = np.zeros((n, n))\n",
        "    fx = f(x, *args)\n",
        "    \n",
        "    for i in range(n):\n",
        "        x_i = x.copy()\n",
        "        x_i[i] += eps\n",
        "        fi = f(x_i, *args)\n",
        "        \n",
        "        for j in range(n):\n",
        "            x_j = x.copy()\n",
        "            x_j[j] += eps\n",
        "            fj = f(x_j, *args)\n",
        "            \n",
        "            x_ij = x.copy()\n",
        "            x_ij[i] += eps\n",
        "            x_ij[j] += eps\n",
        "            fij = f(x_ij, *args)\n",
        "            \n",
        "            hessian[i, j] = (fij - fi - fj + fx) / (eps * eps)\n",
        "    \n",
        "    return hessian\n",
        "\n",
        "# Compute the Hessian at the optimum\n",
        "hessian = compute_hessian(neg_mnl_loglik, beta_mle, 1e-5, mnl_data)\n",
        "\n",
        "# Calculate standard errors from the Hessian matrix\n",
        "se_mle = np.sqrt(np.diag(linalg.inv(hessian)))\n",
        "\n",
        "# Calculate 95% confidence intervals\n",
        "ci_lower = beta_mle - 1.96 * se_mle\n",
        "ci_upper = beta_mle + 1.96 * se_mle"
      ],
      "id": "mle-estimation-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: mle-results-table-python\n",
        "#| echo: true\n",
        "#| tbl-cap: Maximum Likelihood Estimates\n",
        "\n",
        "# Organize results into a DataFrame\n",
        "mle_table = pd.DataFrame({\n",
        "    'Parameter': param_names,\n",
        "    'Estimate': np.round(beta_mle, 3),\n",
        "    'Std_Error': np.round(se_mle, 3),\n",
        "    'CI_Lower': np.round(ci_lower, 3),\n",
        "    'CI_Upper': np.round(ci_upper, 3)\n",
        "})\n",
        "\n",
        "# Display the MLE results\n",
        "mle_table"
      ],
      "id": "mle-results-table-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table presents our maximum likelihood estimates for the four parameters in the MNL model. \n",
        "The estimates are close to the true values used in the simulation: \n",
        "$\\beta_\\text{netflix} = 1.0$, $\\beta_\\text{prime} = 0.5$, $\\beta_\\text{ads} = -0.8$, and $\\beta_\\text{price} = -0.1$. The narrow confidence intervals indicate that our estimates are precise, which is expected given the well-structured simulated data.\n",
        "\n",
        "\n",
        "\n",
        "## 5. Estimation via Bayesian Methods\n",
        "\n",
        "Here I'll implement a Metropolis-Hastings MCMC sampler for Bayesian estimation:"
      ],
      "id": "0b71b8a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: bayes-log-posterior-python\n",
        "#| echo: true\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Define the log-posterior function (log-likelihood + log-prior)\n",
        "def log_posterior(beta, data):\n",
        "    # Calculate log-likelihood\n",
        "    log_lik = mnl_loglik(beta, data)\n",
        "    \n",
        "    # Calculate log-prior\n",
        "    # N(0,5) priors for the betas on binary variables\n",
        "    log_prior_netflix = norm.logpdf(beta[0], 0, 5)\n",
        "    log_prior_prime = norm.logpdf(beta[1], 0, 5)\n",
        "    log_prior_ads = norm.logpdf(beta[2], 0, 5)\n",
        "    # N(0,1) prior for the price beta\n",
        "    log_prior_price = norm.logpdf(beta[3], 0, 1)\n",
        "    \n",
        "    # Sum up log-priors\n",
        "    log_prior = log_prior_netflix + log_prior_prime + log_prior_ads + log_prior_price\n",
        "    \n",
        "    # Return log-posterior\n",
        "    return log_lik + log_prior"
      ],
      "id": "bayes-log-posterior-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: metropolis-hastings-python\n",
        "#| echo: true\n",
        "\n",
        "# Metropolis-Hastings MCMC sampler\n",
        "def metropolis_hastings(data, n_steps=11000, burnin=1000):\n",
        "    # Initialize the chain at the MLE estimates for faster convergence\n",
        "    beta_current = beta_mle.copy()\n",
        "    \n",
        "    # Initialize storage for MCMC samples\n",
        "    beta_samples = np.zeros((n_steps, 4))\n",
        "    beta_samples[0, :] = beta_current\n",
        "    \n",
        "    # Proposal distribution standard deviations\n",
        "    proposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n",
        "    \n",
        "    # Track acceptance rate\n",
        "    accepts = 0\n",
        "    \n",
        "    # Run the MCMC algorithm\n",
        "    for s in range(1, n_steps):\n",
        "        # Generate proposal\n",
        "        beta_proposal = beta_current + np.random.normal(0, proposal_sd, 4)\n",
        "        \n",
        "        # Calculate log acceptance ratio\n",
        "        log_r = log_posterior(beta_proposal, data) - log_posterior(beta_current, data)\n",
        "        \n",
        "        # Accept or reject\n",
        "        if np.log(np.random.random()) < log_r:\n",
        "            beta_current = beta_proposal.copy()\n",
        "            accepts += 1\n",
        "        \n",
        "        # Store the current state\n",
        "        beta_samples[s, :] = beta_current\n",
        "    \n",
        "    # Calculate acceptance rate\n",
        "    acceptance_rate = accepts / (n_steps - 1)\n",
        "    \n",
        "    # Return results\n",
        "    return {\n",
        "        'samples': beta_samples,\n",
        "        'post_burnin': beta_samples[burnin:, :],\n",
        "        'acceptance_rate': acceptance_rate\n",
        "    }"
      ],
      "id": "metropolis-hastings-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: run-mcmc-python\n",
        "#| echo: true\n",
        "\n",
        "# Run the MCMC sampler\n",
        "np.random.seed(456)  # For reproducibility\n",
        "mcmc_results = metropolis_hastings(mnl_data)\n",
        "\n",
        "# Report acceptance rate\n",
        "print(f\"MCMC acceptance rate: {mcmc_results['acceptance_rate']:.3f}\")\n",
        "\n",
        "# Extract post-burnin samples\n",
        "posterior_samples = mcmc_results['post_burnin']"
      ],
      "id": "run-mcmc-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's examine the trace plot and histogram of the posterior distribution for the Netflix parameter:"
      ],
      "id": "70c3925b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 10,
        "fig-height": 5
      },
      "source": [
        "#| label: trace-hist-plots-python\n",
        "#| echo: true\n",
        "#| fig-cap: Trace Plot and Posterior Distribution for Netflix Parameter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create trace plot and histogram for the Netflix parameter\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Trace plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(posterior_samples[:, 0])\n",
        "plt.xlabel('Iteration (post-burnin)')\n",
        "plt.ylabel('Parameter Value')\n",
        "plt.title('Trace Plot: Netflix Parameter')\n",
        "\n",
        "# Histogram\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(posterior_samples[:, 0], bins=30, color='skyblue', edgecolor='white')\n",
        "plt.axvline(np.mean(posterior_samples[:, 0]), color='red', linewidth=2)\n",
        "plt.axvline(np.percentile(posterior_samples[:, 0], 2.5), color='darkred', linewidth=2, linestyle='--')\n",
        "plt.axvline(np.percentile(posterior_samples[:, 0], 97.5), color='darkred', linewidth=2, linestyle='--')\n",
        "plt.xlabel('Parameter Value')\n",
        "plt.title('Posterior Distribution: Netflix Parameter')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "trace-hist-plots-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The trace plot shows good mixing of the Markov chain, indicating efficient exploration of the parameter space. The histogram shows the posterior distribution is approximately normally distributed and centered close to the true value of 1.0.\n",
        "\n",
        "Let's calculate and report the posterior summaries:"
      ],
      "id": "e4d59eb8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: bayes-summary-stats-python\n",
        "#| echo: true\n",
        "\n",
        "# Calculate posterior summary statistics\n",
        "posterior_means = np.mean(posterior_samples, axis=0)\n",
        "posterior_sds = np.std(posterior_samples, axis=0)\n",
        "posterior_ci = np.percentile(posterior_samples, [2.5, 97.5], axis=0).T"
      ],
      "id": "bayes-summary-stats-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: bayes-results-table-python\n",
        "#| echo: true\n",
        "#| tbl-cap: Bayesian Posterior Estimates\n",
        "\n",
        "# Create a summary table\n",
        "bayes_table = pd.DataFrame({\n",
        "    'Parameter': param_names,\n",
        "    'Mean': np.round(posterior_means, 3),\n",
        "    'Std_Dev': np.round(posterior_sds, 3),\n",
        "    'CI_Lower': np.round(posterior_ci[:, 0], 3),\n",
        "    'CI_Upper': np.round(posterior_ci[:, 1], 3)\n",
        "})\n",
        "\n",
        "# Display the Bayesian results\n",
        "bayes_table"
      ],
      "id": "bayes-results-table-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: comparison-table-python\n",
        "#| echo: true\n",
        "#| tbl-cap: 'Comparison: MLE vs Bayesian Estimates'\n",
        "\n",
        "# Compare with MLE results\n",
        "comparison_table = pd.DataFrame({\n",
        "    'Parameter': param_names,\n",
        "    'MLE': np.round(beta_mle, 3),\n",
        "    'MLE_CI_Lower': np.round(ci_lower, 3),\n",
        "    'MLE_CI_Upper': np.round(ci_upper, 3),\n",
        "    'Bayes_Mean': np.round(posterior_means, 3),\n",
        "    'Bayes_CI_Lower': np.round(posterior_ci[:, 0], 3),\n",
        "    'Bayes_CI_Upper': np.round(posterior_ci[:, 1], 3)\n",
        "})\n",
        "\n",
        "# Display the comparison\n",
        "comparison_table"
      ],
      "id": "comparison-table-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Bayesian estimates are very similar to the MLE estimates, which is expected given the large sample size and the relatively uninformative priors. Both methods recover the true parameter values quite well, with the posteriors showing slightly wider credible intervals compared to the confidence intervals from MLE.\n",
        "\n",
        "\n",
        "\n",
        "## 6. Discussion\n",
        "\n",
        "The parameter estimates from both the MLE and Bayesian approaches align closely with the true values used in the simulation ($\\beta_\\text{netflix} = 1.0$, $\\beta_\\text{prime} = 0.5$, $\\beta_\\text{ads} = -0.8$, $\\beta_\\text{price} = -0.1$). This confirms that our estimation methods are working correctly.\n",
        "\n",
        "The interpretation of these parameters provides meaningful insights into consumer preferences:\n",
        "\n",
        "Brand preferences: The positive and significant coefficients for Netflix (approximately 1.0) and Prime (approximately 0.5) indicate that consumers prefer these brands over Hulu (the reference level). Further, the fact that $\\beta_\\text{netflix} > \\beta_\\text{prime}$ means that, all else equal, consumers have a stronger preference for Netflix than for Amazon Prime. In particular, the odds of choosing Netflix over Hulu (if all other attributes are identical) is approximately $e^{1.0} = 2.7$, while the odds of choosing Prime over Hulu is approximately $e^{0.5} = 1.6$.\n",
        "\n",
        "Ad preference: The negative coefficient for ads (approximately -0.8) indicates that, as expected, consumers dislike advertising in their streaming services. The presence of ads reduces utility and thus decreases the probability of choosing a service with ads, all else equal.\n",
        "\n",
        "Price sensitivity: The negative coefficient for price (approximately -0.1) reflects that consumers are price-sensitive. As the price increases, the utility decreases, and consequently, the probability of choosing that service decreases. This makes economic sense as consumers typically prefer lower prices.\n",
        "\n",
        "To extend this model to a multi-level (hierarchical) framework, several key changes would be required:\n",
        "\n",
        "Parameter heterogeneity: Instead of assuming all respondents have the same preferences ($\\beta$s), we would model individual-level parameters that vary across respondents according to a distribution:\n",
        "$\\beta_i \\sim \\text{MVN}(\\mu_\\beta, \\Sigma_\\beta)$\n",
        "\n",
        "where $\\beta_i$ represents the vector of preference parameters for respondent i, $\\mu_\\beta$ is the vector of population means, and $\\Sigma_\\beta$ is the covariance matrix capturing heterogeneity across respondents.\n",
        "\n",
        "Estimation approach: The Bayesian MCMC approach would need to be modified to sample both the individual-level parameters ($\\beta_i$) and the population-level hyperparameters ($\\mu_\\beta$ and $\\Sigma_\\beta$). This typically involves using a Gibbs sampler with Metropolis-Hastings steps.\n",
        "\n",
        "Data structure: The data preparation would remain similar, but we would need to keep track of respondent identities more carefully to model the within-respondent correlation in choices.\n",
        "\n",
        "Computational complexity: The model would become significantly more complex, with hundreds of parameters to estimate (4 parameters per respondent × 100 respondents, plus population parameters), requiring more efficient MCMC algorithms and potentially more computational resources.\n",
        "\n",
        "Prior specifications: We would need to specify priors not only for the mean parameters but also for the covariance matrix, typically using an Inverse-Wishart distribution."
      ],
      "id": "361bb5ea"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/conda/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}