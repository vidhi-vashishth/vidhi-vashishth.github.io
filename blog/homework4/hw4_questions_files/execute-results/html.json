{
  "hash": "4c576635f1467d6091a8a04433e33402",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning\"\nauthor: \"Vidhi Vashishth\"\ndate: today\nexecute:\n  timeout: 300  # Increase timeout to 5 minutes\n  error: true   # Continue rendering even if there are errors\n  warning: true # Show warnings\n---\n\n## 1b. Latent-Class MNL\n\nIn this analysis, I implement a latent-class multinomial logit (MNL) model on yogurt purchase data. The latent-class approach allows us to identify distinct consumer segments with different preferences, rather than assuming all consumers have identical preferences as in the standard MNL model.\n\n::: {#check-dependencies .cell execution_count=1}\n``` {.python .cell-code}\nimport sys\nimport subprocess\n\n# Check for required packages\nrequired_packages = ['scipy', 'sklearn', 'pandas', 'numpy', 'matplotlib', 'seaborn']\n\nfor package in required_packages:\n    try:\n        __import__(package)\n        print(f\"✓ {package} is installed\")\n    except ImportError:\n        print(f\"✗ {package} is not installed. Installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✓ scipy is installed\n✓ sklearn is installed\n✓ pandas is installed\n✓ numpy is installed\n✓ matplotlib is installed\n✓ seaborn is installed\n```\n:::\n:::\n\n\n::: {#data-loading-and-exploration .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\nfrom scipy.stats import multivariate_normal\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load the yogurt dataset\nyogurt_data = pd.read_csv('blog/homework4/yogurt_data.csv')\n\n# Display basic information about the dataset\nprint(\"Dataset shape:\", yogurt_data.shape)\nprint(\"\\nFirst few rows:\")\nprint(yogurt_data.head())\nprint(\"\\nColumn names:\")\nprint(yogurt_data.columns.tolist())\n\n# Analyze purchase patterns\npurchase_counts = (yogurt_data[['y1', 'y2', 'y3', 'y4']].sum())\nprint(\"\\nPurchase counts by product:\")\nprint(purchase_counts)\n\n# Visualize purchase distribution\nplt.figure(figsize=(8, 5))\npurchase_counts.plot(kind='bar', color='skyblue', edgecolor='black')\nplt.title('Distribution of Yogurt Purchases by Product', fontsize=14)\nplt.xlabel('Product')\nplt.ylabel('Number of Purchases')\nplt.xticks(rotation=0)\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[2], line 12</span>\n<span class=\"ansi-green-fg ansi-bold\">      9</span> warnings<span style=\"color:rgb(98,98,98)\">.</span>filterwarnings(<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">ignore</span><span style=\"color:rgb(175,0,0)\">'</span>)\n<span class=\"ansi-green-fg ansi-bold\">     11</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Load the yogurt dataset</span>\n<span class=\"ansi-green-fg\">---&gt; 12</span> yogurt_data <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">pd</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">read_csv</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">blog/homework4/yogurt_data.csv</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     14</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Display basic information about the dataset</span>\n<span class=\"ansi-green-fg ansi-bold\">     15</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">Dataset shape:</span><span style=\"color:rgb(175,0,0)\">\"</span>, yogurt_data<span style=\"color:rgb(98,98,98)\">.</span>shape)\n\nFile <span class=\"ansi-green-fg\">/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026</span>, in <span class=\"ansi-cyan-fg\">read_csv</span><span class=\"ansi-blue-fg\">(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1013</span> kwds_defaults <span style=\"color:rgb(98,98,98)\">=</span> _refine_defaults_read(\n<span class=\"ansi-green-fg ansi-bold\">   1014</span>     dialect,\n<span class=\"ansi-green-fg ansi-bold\">   1015</span>     delimiter,\n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1022</span>     dtype_backend<span style=\"color:rgb(98,98,98)\">=</span>dtype_backend,\n<span class=\"ansi-green-fg ansi-bold\">   1023</span> )\n<span class=\"ansi-green-fg ansi-bold\">   1024</span> kwds<span style=\"color:rgb(98,98,98)\">.</span>update(kwds_defaults)\n<span class=\"ansi-green-fg\">-&gt; 1026</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">_read</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">filepath_or_buffer</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">kwds</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620</span>, in <span class=\"ansi-cyan-fg\">_read</span><span class=\"ansi-blue-fg\">(filepath_or_buffer, kwds)</span>\n<span class=\"ansi-green-fg ansi-bold\">    617</span> _validate_names(kwds<span style=\"color:rgb(98,98,98)\">.</span>get(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">names</span><span style=\"color:rgb(175,0,0)\">\"</span>, <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>))\n<span class=\"ansi-green-fg ansi-bold\">    619</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create the parser.</span>\n<span class=\"ansi-green-fg\">--&gt; 620</span> parser <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">TextFileReader</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">filepath_or_buffer</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwds</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    622</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> chunksize <span style=\"font-weight:bold;color:rgb(175,0,255)\">or</span> iterator:\n<span class=\"ansi-green-fg ansi-bold\">    623</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> parser\n\nFile <span class=\"ansi-green-fg\">/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620</span>, in <span class=\"ansi-cyan-fg\">TextFileReader.__init__</span><span class=\"ansi-blue-fg\">(self, f, engine, **kwds)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1617</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>options[<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">has_index_names</span><span style=\"color:rgb(175,0,0)\">\"</span>] <span style=\"color:rgb(98,98,98)\">=</span> kwds[<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">has_index_names</span><span style=\"color:rgb(175,0,0)\">\"</span>]\n<span class=\"ansi-green-fg ansi-bold\">   1619</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>handles: IOHandles <span style=\"color:rgb(98,98,98)\">|</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span> <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg\">-&gt; 1620</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_engine <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_make_engine</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">f</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">engine</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">/opt/conda/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880</span>, in <span class=\"ansi-cyan-fg\">TextFileReader._make_engine</span><span class=\"ansi-blue-fg\">(self, f, engine)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1878</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">b</span><span style=\"color:rgb(175,0,0)\">\"</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> mode:\n<span class=\"ansi-green-fg ansi-bold\">   1879</span>         mode <span style=\"color:rgb(98,98,98)\">+</span><span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">b</span><span style=\"color:rgb(175,0,0)\">\"</span>\n<span class=\"ansi-green-fg\">-&gt; 1880</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>handles <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">get_handle</span><span class=\"ansi-yellow-bg\">(</span>\n<span class=\"ansi-green-fg ansi-bold\">   1881</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">f</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1882</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">mode</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1883</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">encoding</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">options</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">get</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">encoding</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1884</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">compression</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">options</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">get</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">compression</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1885</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">memory_map</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">options</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">get</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">memory_map</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">False</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1886</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">is_text</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">is_text</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1887</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">errors</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">options</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">get</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">encoding_errors</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">strict</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1888</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">storage_options</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">options</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">get</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">storage_options</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"font-weight:bold;color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">None</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">   1889</span> <span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1890</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">assert</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>handles <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg ansi-bold\">   1891</span> f <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>handles<span style=\"color:rgb(98,98,98)\">.</span>handle\n\nFile <span class=\"ansi-green-fg\">/opt/conda/lib/python3.12/site-packages/pandas/io/common.py:873</span>, in <span class=\"ansi-cyan-fg\">get_handle</span><span class=\"ansi-blue-fg\">(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)</span>\n<span class=\"ansi-green-fg ansi-bold\">    868</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">elif</span> <span style=\"color:rgb(0,135,0)\">isinstance</span>(handle, <span style=\"color:rgb(0,135,0)\">str</span>):\n<span class=\"ansi-green-fg ansi-bold\">    869</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Check whether the filename is to be opened in binary mode.</span>\n<span class=\"ansi-green-fg ansi-bold\">    870</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Binary mode does not support 'encoding' and 'newline'.</span>\n<span class=\"ansi-green-fg ansi-bold\">    871</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> ioargs<span style=\"color:rgb(98,98,98)\">.</span>encoding <span style=\"font-weight:bold;color:rgb(175,0,255)\">and</span> <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">b</span><span style=\"color:rgb(175,0,0)\">\"</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> ioargs<span style=\"color:rgb(98,98,98)\">.</span>mode:\n<span class=\"ansi-green-fg ansi-bold\">    872</span>         <span style=\"font-style:italic;color:rgb(95,135,135)\"># Encoding</span>\n<span class=\"ansi-green-fg\">--&gt; 873</span>         handle <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">open</span><span class=\"ansi-yellow-bg\">(</span>\n<span class=\"ansi-green-fg ansi-bold\">    874</span> <span class=\"ansi-yellow-bg\">            </span><span class=\"ansi-yellow-bg\">handle</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    875</span> <span class=\"ansi-yellow-bg\">            </span><span class=\"ansi-yellow-bg\">ioargs</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">mode</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    876</span> <span class=\"ansi-yellow-bg\">            </span><span class=\"ansi-yellow-bg\">encoding</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">ioargs</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">encoding</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    877</span> <span class=\"ansi-yellow-bg\">            </span><span class=\"ansi-yellow-bg\">errors</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">errors</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    878</span> <span class=\"ansi-yellow-bg\">            </span><span class=\"ansi-yellow-bg\">newline</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">\"</span><span class=\"ansi-yellow-bg\">,</span>\n<span class=\"ansi-green-fg ansi-bold\">    879</span> <span class=\"ansi-yellow-bg\">        </span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    880</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg ansi-bold\">    881</span>         <span style=\"font-style:italic;color:rgb(95,135,135)\"># Binary mode</span>\n<span class=\"ansi-green-fg ansi-bold\">    882</span>         handle <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">open</span>(handle, ioargs<span style=\"color:rgb(98,98,98)\">.</span>mode)\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: 'blog/homework4/yogurt_data.csv'</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#data-reshaping-from-wide-to-long-format .cell execution_count=3}\n``` {.python .cell-code}\ndef reshape_yogurt_data(df):\n    \"\"\"\n    Reshape yogurt data from wide to long format.\n    Each row becomes one alternative for one choice occasion.\n    \"\"\"\n    long_data = []\n    \n    for idx, row in df.iterrows():\n        consumer_id = row['id']\n        \n        # For each alternative (yogurt 1-4)\n        for j in range(1, 5):\n            choice = row[f'y{j}']\n            feature = row[f'f{j}']\n            price = row[f'p{j}']\n            \n            long_data.append({\n                'id': consumer_id,\n                'alternative': j,\n                'choice': choice,\n                'feature': feature,\n                'price': price,\n                'choice_id': idx  # to track choice occasions\n            })\n    \n    return pd.DataFrame(long_data)\n\n# Reshape the data\nlong_data = reshape_yogurt_data(yogurt_data)\nprint(\"\\nReshaped data shape:\", long_data.shape)\nprint(\"\\nFirst 8 rows (2 choice occasions):\")\nprint(long_data.head(8))\n\n# Verify reshaping\nn_choices = yogurt_data.shape[0]\nn_alternatives = 4\nprint(f\"\\nOriginal data: {n_choices} choice occasions\")\nprint(f\"Long format: {long_data.shape[0]} rows ({n_choices} × {n_alternatives} alternatives)\")\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[3], line 29</span>\n<span class=\"ansi-green-fg ansi-bold\">     26</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> pd<span style=\"color:rgb(98,98,98)\">.</span>DataFrame(long_data)\n<span class=\"ansi-green-fg ansi-bold\">     28</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Reshape the data</span>\n<span class=\"ansi-green-fg\">---&gt; 29</span> long_data <span style=\"color:rgb(98,98,98)\">=</span> reshape_yogurt_data(<span class=\"ansi-yellow-bg\">yogurt_data</span>)\n<span class=\"ansi-green-fg ansi-bold\">     30</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">Reshaped data shape:</span><span style=\"color:rgb(175,0,0)\">\"</span>, long_data<span style=\"color:rgb(98,98,98)\">.</span>shape)\n<span class=\"ansi-green-fg ansi-bold\">     31</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">First 8 rows (2 choice occasions):</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'yogurt_data' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#standard-mnl-model-implementation .cell execution_count=4}\n``` {.python .cell-code}\ndef mnl_log_likelihood(params, X, y, n_alts=4):\n    \"\"\"\n    Calculate log-likelihood for standard MNL model.\n    \n    Parameters:\n    - params: array of parameters [price_coef, feature_coef]\n    - X: feature matrix (n_obs × n_features)\n    - y: choice indicators (n_obs,)\n    - n_alts: number of alternatives per choice occasion\n    \"\"\"\n    beta_price = params[0]\n    beta_feature = params[1]\n    \n    n_obs = len(y)\n    n_choices = n_obs // n_alts\n    \n    ll = 0\n    for i in range(n_choices):\n        start_idx = i * n_alts\n        end_idx = (i + 1) * n_alts\n        \n        # Calculate utilities for all alternatives in this choice set\n        utilities = (beta_price * X[start_idx:end_idx, 0] + \n                    beta_feature * X[start_idx:end_idx, 1])\n        \n        # Find which alternative was chosen\n        choice_idx = np.where(y[start_idx:end_idx] == 1)[0][0]\n        \n        # Log-likelihood contribution\n        ll += utilities[choice_idx] - logsumexp(utilities)\n    \n    return -ll  # Return negative for minimization\n\n# Prepare data for MNL estimation\nX = long_data[['price', 'feature']].values\ny = long_data['choice'].values\n\n# Estimate standard MNL\ninit_params = [0, 0]  # Initial values for price and feature coefficients\nresult_mnl = minimize(mnl_log_likelihood, init_params, args=(X, y), \n                     method='BFGS', options={'disp': False})\n\nprint(\"\\n### Standard MNL Results ###\")\nprint(f\"Price coefficient: {result_mnl.x[0]:.4f}\")\nprint(f\"Feature coefficient: {result_mnl.x[1]:.4f}\")\nprint(f\"Log-likelihood: {-result_mnl.fun:.4f}\")\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[4], line 35</span>\n<span class=\"ansi-green-fg ansi-bold\">     32</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(98,98,98)\">-</span>ll  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Return negative for minimization</span>\n<span class=\"ansi-green-fg ansi-bold\">     34</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Prepare data for MNL estimation</span>\n<span class=\"ansi-green-fg\">---&gt; 35</span> X <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">long_data</span>[[<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">price</span><span style=\"color:rgb(175,0,0)\">'</span>, <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">feature</span><span style=\"color:rgb(175,0,0)\">'</span>]]<span style=\"color:rgb(98,98,98)\">.</span>values\n<span class=\"ansi-green-fg ansi-bold\">     36</span> y <span style=\"color:rgb(98,98,98)\">=</span> long_data[<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">choice</span><span style=\"color:rgb(175,0,0)\">'</span>]<span style=\"color:rgb(98,98,98)\">.</span>values\n<span class=\"ansi-green-fg ansi-bold\">     38</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Estimate standard MNL</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'long_data' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#latent-class-mnl-implementation .cell execution_count=5}\n``` {.python .cell-code}\nclass LatentClassMNL:\n    \"\"\"\n    Latent-class multinomial logit model using EM algorithm.\n    \"\"\"\n    \n    def __init__(self, n_classes, n_alts=4, max_iter=100, tol=1e-6):\n        self.n_classes = n_classes\n        self.n_alts = n_alts\n        self.max_iter = max_iter\n        self.tol = tol\n        self.fitted = False\n        \n    def initialize_params(self, n_features):\n        \"\"\"Initialize parameters randomly.\"\"\"\n        # Class-specific choice parameters\n        self.betas = np.random.randn(self.n_classes, n_features)\n        \n        # Class membership probabilities (equal initially)\n        self.class_probs = np.ones(self.n_classes) / self.n_classes\n        \n    def e_step(self, X, y, n_choices):\n        \"\"\"\n        E-step: Calculate posterior class membership probabilities.\n        \"\"\"\n        posteriors = np.zeros((n_choices, self.n_classes))\n        \n        for c in range(self.n_classes):\n            # Calculate likelihood for each choice under class c\n            for i in range(n_choices):\n                start_idx = i * self.n_alts\n                end_idx = (i + 1) * self.n_alts\n                \n                # Utilities for class c\n                utilities = X[start_idx:end_idx] @ self.betas[c]\n                \n                # Find chosen alternative\n                choice_idx = np.where(y[start_idx:end_idx] == 1)[0][0]\n                \n                # Choice probability\n                prob = np.exp(utilities[choice_idx] - logsumexp(utilities))\n                posteriors[i, c] = prob * self.class_probs[c]\n        \n        # Normalize posteriors\n        posteriors = posteriors / posteriors.sum(axis=1, keepdims=True)\n        return posteriors\n    \n    def m_step(self, X, y, posteriors, n_choices):\n        \"\"\"\n        M-step: Update parameters given posterior probabilities.\n        \"\"\"\n        # Update class probabilities\n        self.class_probs = posteriors.mean(axis=0)\n        \n        # Update class-specific parameters\n        for c in range(self.n_classes):\n            # Weighted MNL estimation for class c\n            def weighted_ll(params):\n                ll = 0\n                for i in range(n_choices):\n                    start_idx = i * self.n_alts\n                    end_idx = (i + 1) * self.n_alts\n                    \n                    utilities = X[start_idx:end_idx] @ params\n                    choice_idx = np.where(y[start_idx:end_idx] == 1)[0][0]\n                    \n                    ll += posteriors[i, c] * (utilities[choice_idx] - logsumexp(utilities))\n                return -ll\n            \n            result = minimize(weighted_ll, self.betas[c], method='BFGS', \n                            options={'disp': False})\n            self.betas[c] = result.x\n    \n    def fit(self, X, y):\n        \"\"\"\n        Fit latent-class MNL using EM algorithm.\n        \"\"\"\n        n_obs = len(y)\n        n_choices = n_obs // self.n_alts\n        n_features = X.shape[1]\n        \n        # Initialize parameters\n        self.initialize_params(n_features)\n        \n        # EM algorithm\n        prev_ll = -np.inf\n        for iteration in range(self.max_iter):\n            # E-step\n            posteriors = self.e_step(X, y, n_choices)\n            \n            # M-step\n            self.m_step(X, y, posteriors, n_choices)\n            \n            # Calculate log-likelihood\n            ll = self.log_likelihood(X, y, n_choices)\n            \n            # Check convergence\n            if abs(ll - prev_ll) < self.tol:\n                print(f\"Converged after {iteration + 1} iterations\")\n                break\n            prev_ll = ll\n        \n        self.fitted = True\n        self.final_ll = ll\n        self.posteriors = posteriors\n        \n    def log_likelihood(self, X, y, n_choices):\n        \"\"\"Calculate complete data log-likelihood.\"\"\"\n        ll = 0\n        for i in range(n_choices):\n            start_idx = i * self.n_alts\n            end_idx = (i + 1) * self.n_alts\n            \n            # Likelihood for each class\n            class_liks = np.zeros(self.n_classes)\n            for c in range(self.n_classes):\n                utilities = X[start_idx:end_idx] @ self.betas[c]\n                choice_idx = np.where(y[start_idx:end_idx] == 1)[0][0]\n                class_liks[c] = np.exp(utilities[choice_idx] - logsumexp(utilities))\n            \n            # Weighted by class probabilities\n            ll += np.log(np.sum(class_liks * self.class_probs))\n        \n        return ll\n    \n    def get_n_parameters(self):\n        \"\"\"Calculate total number of parameters.\"\"\"\n        n_features = self.betas.shape[1]\n        # Class-specific betas + class probabilities (minus 1 for identification)\n        return self.n_classes * n_features + (self.n_classes - 1)\n```\n:::\n\n\n::: {#estimating-latent-class-models .cell execution_count=6}\n``` {.python .cell-code}\n# Estimate latent-class models for different numbers of classes\nresults = {}\nn_choices = len(yogurt_data)\n\nprint(\"\\n### Estimating Latent-Class Models ###\")\nfor n_classes in [2, 3, 4]:  # Limiting to fewer classes for faster computation\n    print(f\"\\nFitting {n_classes}-class model...\")\n\n    # Fit model with fewer iterations and increased tolerance\n    model = LatentClassMNL(n_classes=n_classes, max_iter=50, tol=1e-4)\n    model.fit(X, y)\n\n    # Calculate BIC\n    ll = model.final_ll\n    k = model.get_n_parameters()\n    bic = -2 * ll + k * np.log(n_choices)\n\n    results[n_classes] = {\n        'model': model,\n        'log_likelihood': ll,\n        'n_parameters': k,\n        'bic': bic,\n        'class_probs': model.class_probs,\n        'betas': model.betas\n    }\n\n    print(f\"  Log-likelihood: {ll:.2f}\")\n    print(f\"  Parameters: {k}\")\n    print(f\"  BIC: {bic:.2f}\")\n    print(f\"  Class sizes: {model.class_probs.round(3)}\")\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[6], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Estimate latent-class models for different numbers of classes</span>\n<span class=\"ansi-green-fg ansi-bold\">      2</span> results <span style=\"color:rgb(98,98,98)\">=</span> {}\n<span class=\"ansi-green-fg\">----&gt; 3</span> n_choices <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">len</span>(<span class=\"ansi-yellow-bg\">yogurt_data</span>)\n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">### Estimating Latent-Class Models ###</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">      6</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> n_classes <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> [<span style=\"color:rgb(98,98,98)\">2</span>, <span style=\"color:rgb(98,98,98)\">3</span>, <span style=\"color:rgb(98,98,98)\">4</span>]:  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Limiting to fewer classes for faster computation</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'yogurt_data' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#cell-Model-selection-using-BIC .cell execution_count=7}\n``` {.python .cell-code}\n# Create BIC comparison plot\nn_classes_list = list(results.keys())\nbic_values = [results[n]['bic'] for n in n_classes_list]\n\nplt.figure(figsize=(10, 6))\nplt.plot(n_classes_list, bic_values, marker='o', markersize=10, \n         linewidth=2, color='darkblue')\nplt.xlabel('Number of Classes', fontsize=12)\nplt.ylabel('BIC', fontsize=12)\nplt.title('Model Selection: BIC vs Number of Latent Classes', fontsize=14)\nplt.grid(True, alpha=0.3)\n\n# Highlight minimum BIC\nmin_bic_idx = np.argmin(bic_values)\noptimal_classes = n_classes_list[min_bic_idx]\nplt.plot(optimal_classes, bic_values[min_bic_idx], 'ro', markersize=15)\nplt.annotate(f'Optimal: {optimal_classes} classes', \n             xy=(optimal_classes, bic_values[min_bic_idx]),\n             xytext=(optimal_classes + 0.3, bic_values[min_bic_idx] + 20),\n             arrowprops=dict(arrowstyle='->', color='red'),\n             fontsize=12, color='red')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n### Model Selection Result ###\")\nprint(f\"Optimal number of classes according to BIC: {optimal_classes}\")\nprint(f\"BIC value: {results[optimal_classes]['bic']:.2f}\")\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[7], line 14</span>\n<span class=\"ansi-green-fg ansi-bold\">     11</span> plt<span style=\"color:rgb(98,98,98)\">.</span>grid(<span style=\"font-weight:bold;color:rgb(0,135,0)\">True</span>, alpha<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">0.3</span>)\n<span class=\"ansi-green-fg ansi-bold\">     13</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Highlight minimum BIC</span>\n<span class=\"ansi-green-fg\">---&gt; 14</span> min_bic_idx <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">np</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">argmin</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">bic_values</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     15</span> optimal_classes <span style=\"color:rgb(98,98,98)\">=</span> n_classes_list[min_bic_idx]\n<span class=\"ansi-green-fg ansi-bold\">     16</span> plt<span style=\"color:rgb(98,98,98)\">.</span>plot(optimal_classes, bic_values[min_bic_idx], <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">ro</span><span style=\"color:rgb(175,0,0)\">'</span>, markersize<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(98,98,98)\">15</span>)\n\nFile <span class=\"ansi-green-fg\">/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:1325</span>, in <span class=\"ansi-cyan-fg\">argmin</span><span class=\"ansi-blue-fg\">(a, axis, out, keepdims)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1238</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"</span>\n<span class=\"ansi-green-fg ansi-bold\">   1239</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">Returns the indices of the minimum values along an axis.</span>\n<span class=\"ansi-green-fg ansi-bold\">   1240</span> \n<span class=\"ansi-green-fg\">   (...)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1322</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">(2, 1, 4)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1323</span> <span style=\"font-style:italic;color:rgb(175,0,0)\">\"\"\"</span>\n<span class=\"ansi-green-fg ansi-bold\">   1324</span> kwds <span style=\"color:rgb(98,98,98)\">=</span> {<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">keepdims</span><span style=\"color:rgb(175,0,0)\">'</span>: keepdims} <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> keepdims <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> np<span style=\"color:rgb(98,98,98)\">.</span>_NoValue <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span> {}\n<span class=\"ansi-green-fg\">-&gt; 1325</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">_wrapfunc</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">a</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">argmin</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">axis</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">axis</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">out</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">out</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwds</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:56</span>, in <span class=\"ansi-cyan-fg\">_wrapfunc</span><span class=\"ansi-blue-fg\">(obj, method, *args, **kwds)</span>\n<span class=\"ansi-green-fg ansi-bold\">     54</span> bound <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">getattr</span>(obj, method, <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>)\n<span class=\"ansi-green-fg ansi-bold\">     55</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> bound <span style=\"font-weight:bold;color:rgb(175,0,255)\">is</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>:\n<span class=\"ansi-green-fg\">---&gt; 56</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">_wrapit</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">obj</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">method</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwds</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     58</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">try</span>:\n<span class=\"ansi-green-fg ansi-bold\">     59</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> bound(<span style=\"color:rgb(98,98,98)\">*</span>args, <span style=\"color:rgb(98,98,98)\">*</span><span style=\"color:rgb(98,98,98)\">*</span>kwds)\n\nFile <span class=\"ansi-green-fg\">/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:45</span>, in <span class=\"ansi-cyan-fg\">_wrapit</span><span class=\"ansi-blue-fg\">(obj, method, *args, **kwds)</span>\n<span class=\"ansi-green-fg ansi-bold\">     43</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">AttributeError</span>:\n<span class=\"ansi-green-fg ansi-bold\">     44</span>     wrap <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">None</span>\n<span class=\"ansi-green-fg\">---&gt; 45</span> result <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">getattr</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">asarray</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">obj</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">method</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwds</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">     46</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> wrap:\n<span class=\"ansi-green-fg ansi-bold\">     47</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"font-weight:bold;color:rgb(175,0,255)\">not</span> <span style=\"color:rgb(0,135,0)\">isinstance</span>(result, mu<span style=\"color:rgb(98,98,98)\">.</span>ndarray):\n\n<span class=\"ansi-red-fg\">ValueError</span>: attempt to get argmin of an empty sequence</pre>\n```\n:::\n\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw4_questions_files/figure-html/model-selection-using-bic-output-2.png){#model-selection-using-bic width=834 height=528}\n:::\n:::\n\n\n::: {#parameter-comparison .cell execution_count=8}\n``` {.python .cell-code}\n# Compare parameters between aggregate MNL and optimal latent-class model\noptimal_model = results[optimal_classes]\n\n# Create comparison visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot 1: Parameter values comparison\nparam_names = ['Price', 'Feature']\nmnl_params = result_mnl.x\n\n# Prepare data for heatmap\ncomparison_data = np.zeros((optimal_classes + 1, len(param_names)))\ncomparison_data[0, :] = mnl_params\n\nfor c in range(optimal_classes):\n    comparison_data[c + 1, :] = optimal_model['betas'][c]\n\n# Create heatmap\nsns.heatmap(comparison_data, \n            xticklabels=param_names,\n            yticklabels=['Aggregate MNL'] + [f'Class {i+1}' for i in range(optimal_classes)],\n            annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n            cbar_kws={'label': 'Parameter Value'},\n            ax=ax1)\nax1.set_title('Parameter Estimates Comparison', fontsize=14)\n\n# Plot 2: Class sizes (for latent-class model)\nclass_labels = [f'Class {i+1}' for i in range(optimal_classes)]\nclass_sizes = optimal_model['class_probs']\n\nax2.pie(class_sizes, labels=class_labels, autopct='%1.1f%%', \n        colors=plt.cm.Set3(range(optimal_classes)))\nax2.set_title(f'Latent Class Sizes ({optimal_classes}-Class Model)', fontsize=14)\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed comparison\nprint(\"\\n### Parameter Comparison ###\")\nprint(\"\\nAggregate MNL:\")\nprint(f\"  Price coefficient: {mnl_params[0]:.4f}\")\nprint(f\"  Feature coefficient: {mnl_params[1]:.4f}\")\n\nprint(f\"\\n{optimal_classes}-Class Latent Model:\")\nfor c in range(optimal_classes):\n    print(f\"\\nClass {c+1} (size: {optimal_model['class_probs'][c]:.1%}):\")\n    print(f\"  Price coefficient: {optimal_model['betas'][c][0]:.4f}\")\n    print(f\"  Feature coefficient: {optimal_model['betas'][c][1]:.4f}\")\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[8], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Compare parameters between aggregate MNL and optimal latent-class model</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> optimal_model <span style=\"color:rgb(98,98,98)\">=</span> results[<span class=\"ansi-yellow-bg\">optimal_classes</span>]\n<span class=\"ansi-green-fg ansi-bold\">      4</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Create comparison visualization</span>\n<span class=\"ansi-green-fg ansi-bold\">      5</span> fig, (ax1, ax2) <span style=\"color:rgb(98,98,98)\">=</span> plt<span style=\"color:rgb(98,98,98)\">.</span>subplots(<span style=\"color:rgb(98,98,98)\">1</span>, <span style=\"color:rgb(98,98,98)\">2</span>, figsize<span style=\"color:rgb(98,98,98)\">=</span>(<span style=\"color:rgb(98,98,98)\">14</span>, <span style=\"color:rgb(98,98,98)\">6</span>))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'optimal_classes' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#interpretation-and-insights .cell execution_count=9}\n``` {.python .cell-code}\n# Calculate and visualize heterogeneity in price sensitivity\nprice_sensitivities = optimal_model['betas'][:, 0]\nfeature_effects = optimal_model['betas'][:, 1]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Price sensitivity distribution\nax1.bar(range(1, optimal_classes + 1), -price_sensitivities, \n        color='coral', edgecolor='black', alpha=0.7)\nax1.axhline(y=-mnl_params[0], color='darkblue', linestyle='--', \n           label='Aggregate MNL', linewidth=2)\nax1.set_xlabel('Class')\nax1.set_ylabel('Price Sensitivity (absolute value)')\nax1.set_title('Price Sensitivity by Class', fontsize=14)\nax1.set_xticks(range(1, optimal_classes + 1))\nax1.legend()\nax1.grid(axis='y', alpha=0.3)\n\n# Feature effect distribution\nax2.bar(range(1, optimal_classes + 1), feature_effects, \n        color='lightgreen', edgecolor='black', alpha=0.7)\nax2.axhline(y=mnl_params[1], color='darkblue', linestyle='--', \n           label='Aggregate MNL', linewidth=2)\nax2.set_xlabel('Class')\nax2.set_ylabel('Feature Effect')\nax2.set_title('Feature/Promotion Effect by Class', fontsize=14)\nax2.set_xticks(range(1, optimal_classes + 1))\nax2.legend()\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[9], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Calculate and visualize heterogeneity in price sensitivity</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> price_sensitivities <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">optimal_model</span>[<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">betas</span><span style=\"color:rgb(175,0,0)\">'</span>][:, <span style=\"color:rgb(98,98,98)\">0</span>]\n<span class=\"ansi-green-fg ansi-bold\">      3</span> feature_effects <span style=\"color:rgb(98,98,98)\">=</span> optimal_model[<span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">betas</span><span style=\"color:rgb(175,0,0)\">'</span>][:, <span style=\"color:rgb(98,98,98)\">1</span>]\n<span class=\"ansi-green-fg ansi-bold\">      5</span> fig, (ax1, ax2) <span style=\"color:rgb(98,98,98)\">=</span> plt<span style=\"color:rgb(98,98,98)\">.</span>subplots(<span style=\"color:rgb(98,98,98)\">1</span>, <span style=\"color:rgb(98,98,98)\">2</span>, figsize<span style=\"color:rgb(98,98,98)\">=</span>(<span style=\"color:rgb(98,98,98)\">12</span>, <span style=\"color:rgb(98,98,98)\">5</span>))\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'optimal_model' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#summary-and-conclusions .cell execution_count=10}\n``` {.python .cell-code}\nprint(\"\\n### Summary ###\")\nprint(f\"1. The aggregate MNL model assumes all consumers have identical preferences.\")\nprint(f\"2. The BIC criterion suggests {optimal_classes} latent classes provide the best model fit.\")\nprint(f\"3. The latent-class model reveals significant heterogeneity in consumer preferences:\")\n\n# Identify key differences between classes\nfor c in range(optimal_classes):\n    price_sens = -optimal_model['betas'][c][0]\n    feature_eff = optimal_model['betas'][c][1]\n    class_size = optimal_model['class_probs'][c]\n    \n    print(f\"\\n   Class {c+1} ({class_size:.1%} of consumers):\")\n    if price_sens > -mnl_params[0] * 1.2:\n        print(f\"   - High price sensitivity\")\n    elif price_sens < -mnl_params[0] * 0.8:\n        print(f\"   - Low price sensitivity\")\n    else:\n        print(f\"   - Moderate price sensitivity\")\n    \n    if feature_eff > mnl_params[1] * 1.2:\n        print(f\"   - Strongly influenced by features/promotions\")\n    elif feature_eff < mnl_params[1] * 0.8:\n        print(f\"   - Weakly influenced by features/promotions\")\n    else:\n        print(f\"   - Moderately influenced by features/promotions\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n### Summary ###\n1. The aggregate MNL model assumes all consumers have identical preferences.\n```\n:::\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[10], line 3</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">\"</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">### Summary ###</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">      2</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">1. The aggregate MNL model assumes all consumers have identical preferences.</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg\">----&gt; 3</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">2. The BIC criterion suggests </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span><span class=\"ansi-yellow-bg\">optimal_classes</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span style=\"color:rgb(175,0,0)\"> latent classes provide the best model fit.</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">      4</span> <span style=\"color:rgb(0,135,0)\">print</span>(<span style=\"color:rgb(175,0,0)\">f</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">3. The latent-class model reveals significant heterogeneity in consumer preferences:</span><span style=\"color:rgb(175,0,0)\">\"</span>)\n<span class=\"ansi-green-fg ansi-bold\">      6</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Identify key differences between classes</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'optimal_classes' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n## 2a. K Nearest Neighbors\nIn this section, I implement the K-Nearest Neighbors (KNN) algorithm from scratch and evaluate its performance on a synthetic binary classification dataset with a non-linear decision boundary. KNN is a simple yet powerful non-parametric algorithm that makes predictions based on the majority class of the k nearest neighbors in the feature space.\n\n::: {#generate-synthetic-dataset .cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport time\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Generate training data\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nx = np.column_stack((x1, x2))\n\n# Define a wiggly boundary using sin function\nboundary = np.sin(4*x1) + x1\ny = (x2 > boundary).astype(int)\n\n# Create training dataframe\ntrain_data = pd.DataFrame({\n    'x1': x1,\n    'x2': x2,\n    'y': y\n})\n\nprint(\"Training dataset shape:\", train_data.shape)\nprint(\"\\nClass distribution in training set:\")\nprint(train_data['y'].value_counts())\nprint(f\"Class balance: {(y==1).sum()/len(y):.1%} positive class\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining dataset shape: (100, 3)\n\nClass distribution in training set:\ny\n1    51\n0    49\nName: count, dtype: int64\nClass balance: 51.0% positive class\n```\n:::\n:::\n\n\n::: {#cell-Visualise-training-data-with-decision-boundary .cell execution_count=12}\n``` {.python .cell-code}\n# Create figure with the wiggly boundary\nplt.figure(figsize=(10, 8))\n\n# Create a dense grid for plotting the true boundary\nx1_boundary = np.linspace(-3, 3, 1000)\nboundary_line = np.sin(4*x1_boundary) + x1_boundary\n\n# Plot the true decision boundary\nplt.plot(x1_boundary, boundary_line, 'k--', linewidth=2, \n         label='True Boundary: y = sin(4x₁) + x₁', alpha=0.7)\n\n# Plot the data points\ncolors = ['blue', 'red']\nlabels = ['Class 0 (y < boundary)', 'Class 1 (y > boundary)']\n\nfor class_val in [0, 1]:\n    mask = y == class_val\n    plt.scatter(x1[mask], x2[mask], c=colors[class_val], \n                label=labels[class_val], alpha=0.6, edgecolor='black', s=50)\n\nplt.xlabel('x₁', fontsize=12)\nplt.ylabel('x₂', fontsize=12)\nplt.title('Training Data with Non-linear Decision Boundary', fontsize=14)\nplt.legend(loc='best')\nplt.grid(True, alpha=0.3)\nplt.xlim(-3.2, 3.2)\nplt.ylim(-3.2, 3.2)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw4_questions_files/figure-html/visualise-training-data-with-decision-boundary-output-1.png){#visualise-training-data-with-decision-boundary width=951 height=757}\n:::\n:::\n\n\n::: {#generate-test-dataset .cell execution_count=13}\n``` {.python .cell-code}\n# Generate test data with different seed\nnp.random.seed(123)  # Different seed for test data\n\nn_test = 100\nx1_test = np.random.uniform(-3, 3, n_test)\nx2_test = np.random.uniform(-3, 3, n_test)\nx_test = np.column_stack((x1_test, x2_test))\n\n# Apply same boundary rule\nboundary_test = np.sin(4*x1_test) + x1_test\ny_test = (x2_test > boundary_test).astype(int)\n\n# Create test dataframe\ntest_data = pd.DataFrame({\n    'x1': x1_test,\n    'x2': x2_test,\n    'y': y_test\n})\n\nprint(\"\\nTest dataset shape:\", test_data.shape)\nprint(\"\\nClass distribution in test set:\")\nprint(test_data['y'].value_counts())\nprint(f\"Class balance: {(y_test==1).sum()/len(y_test):.1%} positive class\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTest dataset shape: (100, 3)\n\nClass distribution in test set:\ny\n1    52\n0    48\nName: count, dtype: int64\nClass balance: 52.0% positive class\n```\n:::\n:::\n\n\n::: {#implement-knn-algorithm-from-scratch .cell execution_count=14}\n``` {.python .cell-code}\ndef euclidean_distance(x1, x2):\n    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n    return np.sqrt(np.sum((x1 - x2)**2))\n\ndef knn_predict_single(X_train, y_train, x_test, k):\n    \"\"\"\n    Predict class for a single test point using KNN.\n    \n    Parameters:\n    - X_train: Training features (n_samples × n_features)\n    - y_train: Training labels (n_samples,)\n    - x_test: Single test point (n_features,)\n    - k: Number of neighbors\n    \n    Returns:\n    - Predicted class (0 or 1)\n    \"\"\"\n    # Calculate distances to all training points\n    distances = []\n    for i in range(len(X_train)):\n        dist = euclidean_distance(X_train[i], x_test)\n        distances.append((dist, y_train[i]))\n    \n    # Sort by distance and get k nearest neighbors\n    distances.sort(key=lambda x: x[0])\n    k_nearest = distances[:k]\n    \n    # Get labels of k nearest neighbors\n    k_labels = [label for _, label in k_nearest]\n    \n    # Return majority vote (with tie-breaking favoring class 1)\n    return 1 if sum(k_labels) >= k/2 else 0\n\ndef knn_predict(X_train, y_train, X_test, k):\n    \"\"\"\n    Predict classes for multiple test points using KNN.\n    \n    Parameters:\n    - X_train: Training features (n_samples × n_features)\n    - y_train: Training labels (n_samples,)\n    - X_test: Test features (n_test_samples × n_features)\n    - k: Number of neighbors\n    \n    Returns:\n    - Array of predictions\n    \"\"\"\n    predictions = []\n    for i in range(len(X_test)):\n        pred = knn_predict_single(X_train, y_train, X_test[i], k)\n        predictions.append(pred)\n    return np.array(predictions)\n\n# Test the implementation with k=5\nk_test = 5\nstart_time = time.time()\ny_pred_manual = knn_predict(x, y, x_test, k=k_test)\nmanual_time = time.time() - start_time\n\nprint(f\"\\nManual KNN implementation (k={k_test}):\")\nprint(f\"Time taken: {manual_time:.4f} seconds\")\nprint(f\"Predictions shape: {y_pred_manual.shape}\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_manual):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nManual KNN implementation (k=5):\nTime taken: 0.0253 seconds\nPredictions shape: (100,)\nAccuracy: 0.9200\n```\n:::\n:::\n\n\n::: {#cell-Verifying-implementation-with-sklearn .cell execution_count=15}\n``` {.python .cell-code}\n# Compare with sklearn's implementation\nknn_sklearn = KNeighborsClassifier(n_neighbors=k_test)\nstart_time = time.time()\nknn_sklearn.fit(x, y)\ny_pred_sklearn = knn_sklearn.predict(x_test)\nsklearn_time = time.time() - start_time\n\nprint(f\"\\nSklearn KNN implementation (k={k_test}):\")\nprint(f\"Time taken: {sklearn_time:.4f} seconds\")\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred_sklearn):.4f}\")\n\n# Check if predictions match\nmatches = np.sum(y_pred_manual == y_pred_sklearn)\nprint(f\"\\nPrediction agreement: {matches}/{len(y_test)} ({matches/len(y_test)*100:.1f}%)\")\n\n# Visualize predictions comparison\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Manual implementation predictions\nax1.scatter(x_test[y_pred_manual==0, 0], x_test[y_pred_manual==0, 1], \n           c='blue', label='Predicted 0', alpha=0.6, edgecolor='black')\nax1.scatter(x_test[y_pred_manual==1, 0], x_test[y_pred_manual==1, 1], \n           c='red', label='Predicted 1', alpha=0.6, edgecolor='black')\nax1.set_title(f'Manual KNN Predictions (k={k_test})', fontsize=14)\nax1.set_xlabel('x₁')\nax1.set_ylabel('x₂')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Sklearn predictions\nax2.scatter(x_test[y_pred_sklearn==0, 0], x_test[y_pred_sklearn==0, 1], \n           c='blue', label='Predicted 0', alpha=0.6, edgecolor='black')\nax2.scatter(x_test[y_pred_sklearn==1, 0], x_test[y_pred_sklearn==1, 1], \n           c='red', label='Predicted 1', alpha=0.6, edgecolor='black')\nax2.set_title(f'Sklearn KNN Predictions (k={k_test})', fontsize=14)\nax2.set_xlabel('x₁')\nax2.set_ylabel('x₂')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSklearn KNN implementation (k=5):\nTime taken: 0.0042 seconds\nAccuracy: 0.9200\n\nPrediction agreement: 100/100 (100.0%)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hw4_questions_files/figure-html/verifying-implementation-with-sklearn-output-2.png){#verifying-implementation-with-sklearn width=1333 height=563}\n:::\n:::\n\n\n::: {#performance-evaluation .cell execution_count=16}\n``` {.python .cell-code}\n# Test for k values from 1 to 30\nk_values = range(1, 31)\naccuracies = []\ntrain_accuracies = []\n\nprint(\"\\nEvaluating KNN for different k values...\")\nfor k in k_values:\n    # Test set accuracy\n    y_pred = knn_predict(x, y, x_test, k)\n    test_acc = accuracy_score(y_test, y_pred)\n    accuracies.append(test_acc)\n    \n    # Training set accuracy (for comparison)\n    y_pred_train = knn_predict(x, y, x, k)\n    train_acc = accuracy_score(y, y_pred_train)\n    train_accuracies.append(train_acc)\n    \n    if k % 5 == 0:\n        print(f\"k={k:2d}: Test Accuracy = {test_acc:.4f}, Train Accuracy = {train_acc:.4f}\")\n\n# Find optimal k\noptimal_k = k_values[np.argmax(accuracies)]\noptimal_accuracy = max(accuracies)\n\nprint(f\"\\nOptimal k value: {optimal_k}\")\nprint(f\"Maximum test accuracy: {optimal_accuracy:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEvaluating KNN for different k values...\nk= 5: Test Accuracy = 0.9200, Train Accuracy = 0.9500\nk=10: Test Accuracy = 0.9100, Train Accuracy = 0.9300\nk=15: Test Accuracy = 0.9300, Train Accuracy = 0.9200\nk=20: Test Accuracy = 0.9100, Train Accuracy = 0.9200\nk=25: Test Accuracy = 0.9000, Train Accuracy = 0.9100\nk=30: Test Accuracy = 0.9200, Train Accuracy = 0.9200\n\nOptimal k value: 1\nMaximum test accuracy: 0.9500\n```\n:::\n:::\n\n\n::: {#cell-Accuracy-by-choice-of-k .cell execution_count=17}\n``` {.python .cell-code}\n# Create accuracy plot\nplt.figure(figsize=(12, 7))\n\n# Plot both training and test accuracies\nplt.plot(k_values, train_accuracies, 'b-', linewidth=2, \n         label='Training Accuracy', marker='o', markersize=4, alpha=0.7)\nplt.plot(k_values, accuracies, 'r-', linewidth=2, \n         label='Test Accuracy', marker='s', markersize=4, alpha=0.7)\n\n# Highlight optimal k\nplt.scatter(optimal_k, optimal_accuracy, color='green', s=200, \n            zorder=5, edgecolor='black', linewidth=2)\nplt.annotate(f'Optimal k={optimal_k}\\nAccuracy={optimal_accuracy:.3f}', \n             xy=(optimal_k, optimal_accuracy),\n             xytext=(optimal_k + 2, optimal_accuracy - 0.03),\n             arrowprops=dict(arrowstyle='->', color='green', linewidth=2),\n             fontsize=12, color='green', fontweight='bold',\n             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.3))\n\nplt.xlabel('k (Number of Neighbors)', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('KNN Performance: Accuracy vs Number of Neighbors', fontsize=14)\nplt.legend(loc='best', fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 31)\nplt.ylim(0.5, 1.02)\n\n# Add shaded region showing overfitting vs underfitting\nplt.axvspan(1, 5, alpha=0.1, color='blue', label='Potential Overfitting')\nplt.axvspan(20, 30, alpha=0.1, color='red', label='Potential Underfitting')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw4_questions_files/figure-html/accuracy-by-choice-of-k-output-1.png){#accuracy-by-choice-of-k width=1143 height=661}\n:::\n:::\n\n\n::: {#cell-Different-decision-boundaries .cell execution_count=18}\n``` {.python .cell-code}\ndef plot_decision_boundary(X_train, y_train, k, ax, title):\n    \"\"\"Plot decision boundary for KNN with given k.\"\"\"\n    # Create mesh\n    h = 0.1  # step size in mesh\n    x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n    y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    # Predict on mesh points\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    Z = knn_predict(X_train, y_train, mesh_points, k)\n    Z = Z.reshape(xx.shape)\n    \n    # Plot decision boundary\n    ax.contourf(xx, yy, Z, alpha=0.4, cmap=ListedColormap(['lightblue', 'lightcoral']))\n    \n    # Plot training points\n    scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n                         cmap=ListedColormap(['blue', 'red']), \n                         edgecolor='black', s=50, alpha=0.8)\n    \n    ax.set_title(title, fontsize=12)\n    ax.set_xlabel('x₁')\n    ax.set_ylabel('x₂')\n    ax.grid(True, alpha=0.3)\n    \n    return scatter\n\n# Visualize decision boundaries for different k values\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.ravel()\n\nk_values_to_plot = [1, 3, 5, 10, 20, optimal_k]\ntitles = [f'k = {k}' if k != optimal_k else f'k = {k} (Optimal)' \n          for k in k_values_to_plot]\n\nfor idx, (k, title) in enumerate(zip(k_values_to_plot, titles)):\n    plot_decision_boundary(x, y, k, axes[idx], title)\n\nplt.suptitle('KNN Decision Boundaries for Different k Values', fontsize=16)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw4_questions_files/figure-html/different-decision-boundaries-output-1.png){#different-decision-boundaries width=1429 height=944}\n:::\n:::\n\n\n::: {#analyze-the-behavior-of-accuracy-with-k .cell execution_count=19}\n``` {.python .cell-code}\nprint(\"\\n### Analysis of Results ###\")\n\n# Find k values with top 5 accuracies\ntop_k_indices = np.argsort(accuracies)[-5:][::-1]\nprint(\"\\nTop 5 k values by test accuracy:\")\nfor i, idx in enumerate(top_k_indices):\n    print(f\"{i+1}. k={k_values[idx]:2d}: Accuracy = {accuracies[idx]:.4f}\")\n\n# Analyze stability of accuracy around optimal k\nk_window = 2\noptimal_idx = optimal_k - 1  # Convert to 0-based index\nwindow_start = max(0, optimal_idx - k_window)\nwindow_end = min(len(accuracies), optimal_idx + k_window + 1)\nwindow_accuracies = accuracies[window_start:window_end]\nstability = np.std(window_accuracies)\n\nprint(f\"\\nStability analysis around optimal k={optimal_k}:\")\nprint(f\"Accuracy std in window [{optimal_k-k_window}, {optimal_k+k_window}]: {stability:.4f}\")\nprint(f\"Average accuracy in window: {np.mean(window_accuracies):.4f}\")\n\n# Bias-variance tradeoff discussion\nprint(\"\\n### Bias-Variance Tradeoff ###\")\nprint(f\"k=1  accuracy: {accuracies[0]:.4f} (Low bias, high variance)\")\nprint(f\"k={optimal_k} accuracy: {optimal_accuracy:.4f} (Optimal tradeoff)\")\nprint(f\"k=30 accuracy: {accuracies[-1]:.4f} (High bias, low variance)\")\n\nprint(\"\\n### Summary ###\")\nprint(f\"1. Successfully implemented KNN algorithm from scratch\")\nprint(f\"2. Implementation verified against sklearn (>{(np.sum(y_pred_manual == y_pred_sklearn)/len(y_test)*100):.0f}% agreement)\")\nprint(f\"3. Optimal k value: {optimal_k} with test accuracy of {optimal_accuracy:.4f}\")\nprint(f\"4. The non-linear decision boundary is well-captured by KNN with appropriate k\")\nprint(f\"\\nKey observations:\")\nprint(f\"- Small k values (1-3) show signs of overfitting with jagged decision boundaries\")\nprint(f\"- Large k values (>20) show signs of underfitting with overly smooth boundaries\")\nprint(f\"- The optimal k={optimal_k} provides a good balance between bias and variance\")\nprint(f\"- The wiggly sin-based boundary is effectively learned by the non-parametric KNN approach\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n### Analysis of Results ###\n\nTop 5 k values by test accuracy:\n1. k= 1: Accuracy = 0.9500\n2. k= 2: Accuracy = 0.9500\n3. k= 6: Accuracy = 0.9400\n4. k=21: Accuracy = 0.9300\n5. k= 3: Accuracy = 0.9300\n\nStability analysis around optimal k=1:\nAccuracy std in window [-1, 3]: 0.0094\nAverage accuracy in window: 0.9433\n\n### Bias-Variance Tradeoff ###\nk=1  accuracy: 0.9500 (Low bias, high variance)\nk=1 accuracy: 0.9500 (Optimal tradeoff)\nk=30 accuracy: 0.9200 (High bias, low variance)\n\n### Summary ###\n1. Successfully implemented KNN algorithm from scratch\n2. Implementation verified against sklearn (>100% agreement)\n3. Optimal k value: 1 with test accuracy of 0.9500\n4. The non-linear decision boundary is well-captured by KNN with appropriate k\n\nKey observations:\n- Small k values (1-3) show signs of overfitting with jagged decision boundaries\n- Large k values (>20) show signs of underfitting with overly smooth boundaries\n- The optimal k=1 provides a good balance between bias and variance\n- The wiggly sin-based boundary is effectively learned by the non-parametric KNN approach\n```\n:::\n:::\n\n\n",
    "supporting": [
      "hw4_questions_files"
    ],
    "filters": [],
    "includes": {}
  }
}