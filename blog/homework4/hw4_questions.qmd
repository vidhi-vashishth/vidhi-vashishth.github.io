---
title: "Machine Learning"
author: "Vidhi Vashishth"
date: today
execute:
  timeout: 300  # Increase timeout to 5 minutes
  error: true   # Continue rendering even if there are errors
  warning: true # Show warnings
---

## 1a. K-Means

In this analysis, I implement the K-means clustering algorithm from scratch and apply it to the Palmer Penguins dataset. K-means is an unsupervised learning algorithm that partitions data into K clusters by iteratively assigning points to the nearest cluster center and updating centers based on the assigned points.

```{python}
#| label: data-loading-and-prep
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')
```

```{python}
# Load the penguins dataset
penguins = pd.read_csv('palmer_penguins.csv')
penguins_clean = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()
print(f"\nDataset shape after cleaning: {penguins_clean.shape}")
print(f"Removed {len(penguins) - len(penguins_clean)} rows with missing values")
```

```{python}
# Display basic information about the dataset
print("\nBasic statistics of the features:")
print(penguins_clean.describe())

X = penguins_clean.values
feature_names = ['Bill Length (mm)', 'Flipper Length (mm)']

# Standardize the features for better clustering performance
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create both scaled and unscaled versions for visualization
X_unscaled = X.copy()
```

```{python}
#| label: visualise-original-data
#| echo: true

plt.figure(figsize=(10, 6))
plt.scatter(X_unscaled[:, 0], X_unscaled[:, 1], alpha=0.6, edgecolors='black', s=50)
plt.xlabel('Bill Length (mm)', fontsize=12)
plt.ylabel('Flipper Length (mm)', fontsize=12)
plt.title('Palmer Penguins: Bill Length vs Flipper Length', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Show the correlation between features
correlation = np.corrcoef(X_unscaled[:, 0], X_unscaled[:, 1])[0, 1]
print(f"\nCorrelation between bill length and flipper length: {correlation:.3f}")
```

```{python}
#| label: k-means-implementation
#| echo: true
class KMeansCustom:
    """
    Custom implementation of K-means clustering algorithm.
    """
    
    def __init__(self, n_clusters=3, max_iters=100, random_state=42):
        self.n_clusters = n_clusters
        self.max_iters = max_iters
        self.random_state = random_state
        self.history = {'centroids': [], 'assignments': [], 'inertia': []}
        
    def initialize_centroids(self, X):
        """Initialize centroids using random data points."""
        np.random.seed(self.random_state)
        n_samples = X.shape[0]
        random_indices = np.random.choice(n_samples, self.n_clusters, replace=False)
        return X[random_indices].copy()
    
    def assign_clusters(self, X, centroids):
        """Assign each point to the nearest centroid."""
        distances = np.zeros((X.shape[0], self.n_clusters))
        
        for k in range(self.n_clusters):
            # Calculate Euclidean distance to each centroid
            distances[:, k] = np.sqrt(np.sum((X - centroids[k])**2, axis=1))
        
        # Assign to closest centroid
        return np.argmin(distances, axis=1)
    
    def update_centroids(self, X, assignments):
        """Update centroids as mean of assigned points."""
        centroids = np.zeros((self.n_clusters, X.shape[1]))
        
        for k in range(self.n_clusters):
            cluster_points = X[assignments == k]
            if len(cluster_points) > 0:
                centroids[k] = cluster_points.mean(axis=0)
            else:
                # If no points assigned, keep the centroid unchanged
                centroids[k] = self.centroids[k]
        
        return centroids
    
    def calculate_inertia(self, X, assignments, centroids):
        """Calculate within-cluster sum of squares."""
        inertia = 0
        for k in range(self.n_clusters):
            cluster_points = X[assignments == k]
            if len(cluster_points) > 0:
                inertia += np.sum((cluster_points - centroids[k])**2)
        return inertia
    
    def fit(self, X):
        """Fit K-means to the data."""
        # Initialize centroids
        self.centroids = self.initialize_centroids(X)
        self.history['centroids'].append(self.centroids.copy())
        
        for iteration in range(self.max_iters):
            # Assign clusters
            assignments = self.assign_clusters(X, self.centroids)
            self.history['assignments'].append(assignments.copy())
            
            # Calculate inertia
            inertia = self.calculate_inertia(X, assignments, self.centroids)
            self.history['inertia'].append(inertia)
            
            # Update centroids
            new_centroids = self.update_centroids(X, assignments)
            
            # Check for convergence
            if np.allclose(self.centroids, new_centroids):
                print(f"Converged after {iteration + 1} iterations")
                break
            
            self.centroids = new_centroids
            self.history['centroids'].append(self.centroids.copy())
        
        self.labels_ = assignments
        self.inertia_ = inertia
        return self
    
    def predict(self, X):
        """Predict cluster assignments for new data."""
        return self.assign_clusters(X, self.centroids)
```

```{python}
#| label: k-means-visualisation
#| echo: true
# Run custom K-means with K=3 and visualize the steps
kmeans_custom = KMeansCustom(n_clusters=3, random_state=42)
kmeans_custom.fit(X_scaled)

# Create visualizations of the algorithm's progress
n_steps = min(6, len(kmeans_custom.history['centroids']))
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for i in range(n_steps):
    ax = axes[i]
    
    # Get data for this iteration
    if i < len(kmeans_custom.history['assignments']):
        assignments = kmeans_custom.history['assignments'][i]
        centroids = kmeans_custom.history['centroids'][min(i, len(kmeans_custom.history['centroids'])-1)]
    else:
        assignments = kmeans_custom.labels_
        centroids = kmeans_custom.centroids
    
    # Plot points colored by assignment
    scatter = ax.scatter(X_unscaled[:, 0], X_unscaled[:, 1], 
                        c=assignments, cmap='viridis', alpha=0.6, 
                        edgecolors='black', s=50)
    
    # Plot centroids
    centroids_unscaled = scaler.inverse_transform(centroids)
    ax.scatter(centroids_unscaled[:, 0], centroids_unscaled[:, 1], 
              c='red', marker='*', s=300, edgecolors='black', linewidth=2)
    
    # Add title
    if i == 0:
        ax.set_title(f'Initial State', fontsize=12)
    elif i < len(kmeans_custom.history['assignments']):
        ax.set_title(f'Iteration {i}', fontsize=12)
    else:
        ax.set_title(f'Final State', fontsize=12)
    
    ax.set_xlabel('Bill Length (mm)')
    ax.set_ylabel('Flipper Length (mm)')
    ax.grid(True, alpha=0.3)

plt.suptitle('K-Means Algorithm Progress (K=3)', fontsize=16)
plt.tight_layout()
plt.show()

# Plot inertia over iterations
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(kmeans_custom.history['inertia']) + 1), 
         kmeans_custom.history['inertia'], 
         marker='o', linewidth=2, markersize=8)
plt.xlabel('Iteration', fontsize=12)
plt.ylabel('Within-Cluster Sum of Squares (Inertia)', fontsize=12)
plt.title('K-Means Convergence: Inertia vs Iteration', fontsize=14)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

```{python}
#| label: compare-with-inbuilt-sklearn-kmeans
#| echo: true

# Run sklearn's KMeans with same parameters
kmeans_sklearn = KMeans(n_clusters=3, random_state=42, n_init=1)
kmeans_sklearn.fit(X_scaled)

# Compare cluster assignments
agreement = np.sum(kmeans_custom.labels_ == kmeans_sklearn.labels_) / len(kmeans_custom.labels_)
print(f"\nCluster assignment agreement with sklearn: {agreement:.1%}")

# Note: Cluster labels might be permuted, so let's check if the clustering is essentially the same
from scipy.optimize import linear_sum_assignment

def compare_clusterings(labels1, labels2):
    """Compare two clusterings accounting for label permutations."""
    n_clusters = len(np.unique(labels1))
    confusion_matrix = np.zeros((n_clusters, n_clusters))
    
    for i in range(n_clusters):
        for j in range(n_clusters):
            confusion_matrix[i, j] = np.sum((labels1 == i) & (labels2 == j))
    
    # Find optimal assignment
    row_ind, col_ind = linear_sum_assignment(-confusion_matrix)
    
    # Calculate agreement with optimal permutation
    total_agreement = confusion_matrix[row_ind, col_ind].sum()
    return total_agreement / len(labels1)

adjusted_agreement = compare_clusterings(kmeans_custom.labels_, kmeans_sklearn.labels_)
print(f"Adjusted cluster agreement (accounting for label permutation): {adjusted_agreement:.1%}")

# Visualize comparison
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Custom implementation
scatter1 = ax1.scatter(X_unscaled[:, 0], X_unscaled[:, 1], 
                      c=kmeans_custom.labels_, cmap='viridis', 
                      alpha=0.6, edgecolors='black', s=50)
centroids_custom = scaler.inverse_transform(kmeans_custom.centroids)
ax1.scatter(centroids_custom[:, 0], centroids_custom[:, 1], 
           c='red', marker='*', s=300, edgecolors='black', linewidth=2)
ax1.set_title('Custom K-Means Implementation', fontsize=14)
ax1.set_xlabel('Bill Length (mm)')
ax1.set_ylabel('Flipper Length (mm)')
ax1.grid(True, alpha=0.3)

# Sklearn implementation
scatter2 = ax2.scatter(X_unscaled[:, 0], X_unscaled[:, 1], 
                      c=kmeans_sklearn.labels_, cmap='viridis', 
                      alpha=0.6, edgecolors='black', s=50)
centroids_sklearn = scaler.inverse_transform(kmeans_sklearn.cluster_centers_)
ax2.scatter(centroids_sklearn[:, 0], centroids_sklearn[:, 1], 
           c='red', marker='*', s=300, edgecolors='black', linewidth=2)
ax2.set_title('Sklearn K-Means Implementation', fontsize=14)
ax2.set_xlabel('Bill Length (mm)')
ax2.set_ylabel('Flipper Length (mm)')
ax2.grid(True, alpha=0.3)

plt.suptitle('Comparison: Custom vs Sklearn K-Means', fontsize=16)
plt.tight_layout()
plt.show()
```

```{python}
#| label: evaluate-different-clusters
#| echo: true
# Test different values of K
K_values = range(2, 8)
wcss_values = []  # Within-cluster sum of squares
silhouette_values = []

for k in K_values:
    # Custom implementation
    kmeans = KMeansCustom(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    
    # Calculate metrics
    wcss = kmeans.inertia_
    wcss_values.append(wcss)
    
    # Silhouette score (using sklearn for consistency)
    if k > 1:  # Silhouette score requires at least 2 clusters
        silhouette = silhouette_score(X_scaled, kmeans.labels_)
        silhouette_values.append(silhouette)
    
    print(f"K={k}: WCSS={wcss:.2f}, Silhouette={silhouette:.3f}")
```


```{python}
#| label: model-selection
#| echo: true

# Create subplots for both metrics
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Plot 1: WCSS (Elbow Method)
ax1.plot(K_values, wcss_values, marker='o', linewidth=2, markersize=10, color='darkblue')
ax1.set_xlabel('Number of Clusters (K)', fontsize=12)
ax1.set_ylabel('Within-Cluster Sum of Squares', fontsize=12)
ax1.set_title('Elbow Method for Optimal K', fontsize=14)
ax1.grid(True, alpha=0.3)

# Mark the "elbow" (using a simple heuristic)
diffs = np.diff(wcss_values)
diffs2 = np.diff(diffs)
elbow_idx = np.argmax(diffs2) + 2  # +2 because of double differencing
ax1.axvline(x=K_values[elbow_idx], color='red', linestyle='--', alpha=0.7)
ax1.annotate(f'Elbow at K={K_values[elbow_idx]}', 
            xy=(K_values[elbow_idx], wcss_values[elbow_idx]),
            xytext=(K_values[elbow_idx] + 0.5, wcss_values[elbow_idx] + 50),
            arrowprops=dict(arrowstyle='->', color='red'),
            fontsize=12, color='red')

# Plot 2: Silhouette Score
ax2.plot(K_values, silhouette_values, marker='s', linewidth=2, markersize=10, color='darkgreen')
ax2.set_xlabel('Number of Clusters (K)', fontsize=12)
ax2.set_ylabel('Silhouette Score', fontsize=12)
ax2.set_title('Silhouette Score for Different K', fontsize=14)
ax2.grid(True, alpha=0.3)

# Mark the maximum silhouette score
max_silhouette_idx = np.argmax(silhouette_values)
optimal_k_silhouette = K_values[max_silhouette_idx]
ax2.axvline(x=optimal_k_silhouette, color='green', linestyle='--', alpha=0.7)
ax2.annotate(f'Max at K={optimal_k_silhouette}', 
            xy=(optimal_k_silhouette, silhouette_values[max_silhouette_idx]),
            xytext=(optimal_k_silhouette + 0.5, silhouette_values[max_silhouette_idx] - 0.05),
            arrowprops=dict(arrowstyle='->', color='green'),
            fontsize=12, color='green')

plt.tight_layout()
plt.show()
```


```{python}
#| label: clustering-results
#| echo: true

# Show clustering results for all K values
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for idx, k in enumerate(K_values):
    kmeans = KMeansCustom(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    
    ax = axes[idx]
    scatter = ax.scatter(X_unscaled[:, 0], X_unscaled[:, 1], 
                        c=kmeans.labels_, cmap='viridis', 
                        alpha=0.6, edgecolors='black', s=50)
    
    # Plot centroids
    centroids_unscaled = scaler.inverse_transform(kmeans.centroids)
    ax.scatter(centroids_unscaled[:, 0], centroids_unscaled[:, 1], 
              c='red', marker='*', s=300, edgecolors='black', linewidth=2,
              label='Centroids')
    
    ax.set_title(f'K = {k}', fontsize=12)
    ax.set_xlabel('Bill Length (mm)')
    ax.set_ylabel('Flipper Length (mm)')
    ax.grid(True, alpha=0.3)
    
    # Add silhouette score to title
    if k > 1:
        sil_score = silhouette_score(X_scaled, kmeans.labels_)
        ax.set_title(f'K = {k} (Silhouette: {sil_score:.3f})', fontsize=12)

plt.suptitle('K-Means Clustering Results for Different K Values', fontsize=16)
plt.tight_layout()
plt.show()
```

```{python}
#| label: analysis-and-interpretation
#| echo: true

print("\n### Clustering Analysis Summary ###")
print(f"\n1. Elbow Method suggests K = {K_values[elbow_idx]} clusters")
print(f"2. Silhouette Score suggests K = {optimal_k_silhouette} clusters")

# Run final clustering with optimal K
optimal_k = optimal_k_silhouette
final_kmeans = KMeansCustom(n_clusters=optimal_k, random_state=42)
final_kmeans.fit(X_scaled)

# Analyze cluster characteristics
print(f"\n### Final Clustering with K = {optimal_k} ###")
for k in range(optimal_k):
    cluster_mask = final_kmeans.labels_ == k
    cluster_data = X_unscaled[cluster_mask]
    
    print(f"\nCluster {k}:")
    print(f"  Size: {len(cluster_data)} penguins ({len(cluster_data)/len(X_unscaled)*100:.1f}%)")
    print(f"  Mean Bill Length: {cluster_data[:, 0].mean():.1f} mm")
    print(f"  Mean Flipper Length: {cluster_data[:, 1].mean():.1f} mm")
    print(f"  Bill Length Std: {cluster_data[:, 0].std():.1f} mm")
    print(f"  Flipper Length Std: {cluster_data[:, 1].std():.1f} mm")
```

```{python}
#| label: Summary-and-Conclusions
#| echo: true
print("\n### Summary ###")
print("1. Successfully implemented K-means clustering algorithm from scratch")
print("2. The algorithm correctly identifies clusters through iterative optimization")
print(f"3. Comparison with sklearn shows {adjusted_agreement:.1%} agreement in cluster assignments")
print(f"4. Both WCSS (elbow) and Silhouette metrics suggest K={optimal_k} as optimal")
print("\n### Key Insights ###")
print("- The Palmer Penguins dataset shows natural clustering in bill and flipper measurements")
print("- These clusters likely correspond to different penguin species or sex differences")
print("- K-means effectively separates the groups based on these morphological features")
print("- The iterative nature of K-means is clearly visible in the algorithm visualization")
```

## 2a. K Nearest Neighbors
In this section, I implement the K-Nearest Neighbors (KNN) algorithm from scratch and evaluate its performance on a synthetic binary classification dataset with a non-linear decision boundary. KNN is a simple yet powerful non-parametric algorithm that makes predictions based on the majority class of the k nearest neighbors in the feature space.

```{python}
#| label: Generate-Synthetic-Dataset
#| echo: true


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import time

# Set random seed for reproducibility
np.random.seed(42)

# Generate training data
n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
x = np.column_stack((x1, x2))

# Define a wiggly boundary using sin function
boundary = np.sin(4*x1) + x1
y = (x2 > boundary).astype(int)

# Create training dataframe
train_data = pd.DataFrame({
    'x1': x1,
    'x2': x2,
    'y': y
})

print("Training dataset shape:", train_data.shape)
print("\nClass distribution in training set:")
print(train_data['y'].value_counts())
print(f"Class balance: {(y==1).sum()/len(y):.1%} positive class")
```


```{python}
#| label: Visualise-training-data-with-decision-boundary
#| echo: true


# Create figure with the wiggly boundary
plt.figure(figsize=(10, 8))

# Create a dense grid for plotting the true boundary
x1_boundary = np.linspace(-3, 3, 1000)
boundary_line = np.sin(4*x1_boundary) + x1_boundary

# Plot the true decision boundary
plt.plot(x1_boundary, boundary_line, 'k--', linewidth=2, 
         label='True Boundary: y = sin(4x₁) + x₁', alpha=0.7)

# Plot the data points
colors = ['blue', 'red']
labels = ['Class 0 (y < boundary)', 'Class 1 (y > boundary)']

for class_val in [0, 1]:
    mask = y == class_val
    plt.scatter(x1[mask], x2[mask], c=colors[class_val], 
                label=labels[class_val], alpha=0.6, edgecolor='black', s=50)

plt.xlabel('x₁', fontsize=12)
plt.ylabel('x₂', fontsize=12)
plt.title('Training Data with Non-linear Decision Boundary', fontsize=14)
plt.legend(loc='best')
plt.grid(True, alpha=0.3)
plt.xlim(-3.2, 3.2)
plt.ylim(-3.2, 3.2)
plt.tight_layout()
plt.show()
```


```{python}
#| label: Generate-test-dataset
#| echo: true


# Generate test data with different seed
np.random.seed(123)  # Different seed for test data

n_test = 100
x1_test = np.random.uniform(-3, 3, n_test)
x2_test = np.random.uniform(-3, 3, n_test)
x_test = np.column_stack((x1_test, x2_test))

# Apply same boundary rule
boundary_test = np.sin(4*x1_test) + x1_test
y_test = (x2_test > boundary_test).astype(int)

# Create test dataframe
test_data = pd.DataFrame({
    'x1': x1_test,
    'x2': x2_test,
    'y': y_test
})

print("\nTest dataset shape:", test_data.shape)
print("\nClass distribution in test set:")
print(test_data['y'].value_counts())
print(f"Class balance: {(y_test==1).sum()/len(y_test):.1%} positive class")
```


```{python}
#| label: Implement-KNN-algorithm-from-scratch
#| echo: true


def euclidean_distance(x1, x2):
    """Calculate Euclidean distance between two points."""
    return np.sqrt(np.sum((x1 - x2)**2))

def knn_predict_single(X_train, y_train, x_test, k):
    """
    Predict class for a single test point using KNN.
    
    Parameters:
    - X_train: Training features (n_samples × n_features)
    - y_train: Training labels (n_samples,)
    - x_test: Single test point (n_features,)
    - k: Number of neighbors
    
    Returns:
    - Predicted class (0 or 1)
    """
    # Calculate distances to all training points
    distances = []
    for i in range(len(X_train)):
        dist = euclidean_distance(X_train[i], x_test)
        distances.append((dist, y_train[i]))
    
    # Sort by distance and get k nearest neighbors
    distances.sort(key=lambda x: x[0])
    k_nearest = distances[:k]
    
    # Get labels of k nearest neighbors
    k_labels = [label for _, label in k_nearest]
    
    # Return majority vote (with tie-breaking favoring class 1)
    return 1 if sum(k_labels) >= k/2 else 0

def knn_predict(X_train, y_train, X_test, k):
    """
    Predict classes for multiple test points using KNN.
    
    Parameters:
    - X_train: Training features (n_samples × n_features)
    - y_train: Training labels (n_samples,)
    - X_test: Test features (n_test_samples × n_features)
    - k: Number of neighbors
    
    Returns:
    - Array of predictions
    """
    predictions = []
    for i in range(len(X_test)):
        pred = knn_predict_single(X_train, y_train, X_test[i], k)
        predictions.append(pred)
    return np.array(predictions)

# Test the implementation with k=5
k_test = 5
start_time = time.time()
y_pred_manual = knn_predict(x, y, x_test, k=k_test)
manual_time = time.time() - start_time

print(f"\nManual KNN implementation (k={k_test}):")
print(f"Time taken: {manual_time:.4f} seconds")
print(f"Predictions shape: {y_pred_manual.shape}")
print(f"Accuracy: {accuracy_score(y_test, y_pred_manual):.4f}")
```


```{python}
#| label: Verifying-implementation-with-sklearn
#| echo: true


# Compare with sklearn's implementation
knn_sklearn = KNeighborsClassifier(n_neighbors=k_test)
start_time = time.time()
knn_sklearn.fit(x, y)
y_pred_sklearn = knn_sklearn.predict(x_test)
sklearn_time = time.time() - start_time

print(f"\nSklearn KNN implementation (k={k_test}):")
print(f"Time taken: {sklearn_time:.4f} seconds")
print(f"Accuracy: {accuracy_score(y_test, y_pred_sklearn):.4f}")

# Check if predictions match
matches = np.sum(y_pred_manual == y_pred_sklearn)
print(f"\nPrediction agreement: {matches}/{len(y_test)} ({matches/len(y_test)*100:.1f}%)")

# Visualize predictions comparison
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# Manual implementation predictions
ax1.scatter(x_test[y_pred_manual==0, 0], x_test[y_pred_manual==0, 1], 
           c='blue', label='Predicted 0', alpha=0.6, edgecolor='black')
ax1.scatter(x_test[y_pred_manual==1, 0], x_test[y_pred_manual==1, 1], 
           c='red', label='Predicted 1', alpha=0.6, edgecolor='black')
ax1.set_title(f'Manual KNN Predictions (k={k_test})', fontsize=14)
ax1.set_xlabel('x₁')
ax1.set_ylabel('x₂')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Sklearn predictions
ax2.scatter(x_test[y_pred_sklearn==0, 0], x_test[y_pred_sklearn==0, 1], 
           c='blue', label='Predicted 0', alpha=0.6, edgecolor='black')
ax2.scatter(x_test[y_pred_sklearn==1, 0], x_test[y_pred_sklearn==1, 1], 
           c='red', label='Predicted 1', alpha=0.6, edgecolor='black')
ax2.set_title(f'Sklearn KNN Predictions (k={k_test})', fontsize=14)
ax2.set_xlabel('x₁')
ax2.set_ylabel('x₂')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```


```{python}
#| label: Performance-evaluation
#| echo: true


# Test for k values from 1 to 30
k_values = range(1, 31)
accuracies = []
train_accuracies = []

print("\nEvaluating KNN for different k values...")
for k in k_values:
    # Test set accuracy
    y_pred = knn_predict(x, y, x_test, k)
    test_acc = accuracy_score(y_test, y_pred)
    accuracies.append(test_acc)
    
    # Training set accuracy (for comparison)
    y_pred_train = knn_predict(x, y, x, k)
    train_acc = accuracy_score(y, y_pred_train)
    train_accuracies.append(train_acc)
    
    if k % 5 == 0:
        print(f"k={k:2d}: Test Accuracy = {test_acc:.4f}, Train Accuracy = {train_acc:.4f}")

# Find optimal k
optimal_k = k_values[np.argmax(accuracies)]
optimal_accuracy = max(accuracies)

print(f"\nOptimal k value: {optimal_k}")
print(f"Maximum test accuracy: {optimal_accuracy:.4f}")
```


```{python}
#| label: Accuracy-by-choice-of-k
#| echo: true


# Create accuracy plot
plt.figure(figsize=(12, 7))

# Plot both training and test accuracies
plt.plot(k_values, train_accuracies, 'b-', linewidth=2, 
         label='Training Accuracy', marker='o', markersize=4, alpha=0.7)
plt.plot(k_values, accuracies, 'r-', linewidth=2, 
         label='Test Accuracy', marker='s', markersize=4, alpha=0.7)

# Highlight optimal k
plt.scatter(optimal_k, optimal_accuracy, color='green', s=200, 
            zorder=5, edgecolor='black', linewidth=2)
plt.annotate(f'Optimal k={optimal_k}\nAccuracy={optimal_accuracy:.3f}', 
             xy=(optimal_k, optimal_accuracy),
             xytext=(optimal_k + 2, optimal_accuracy - 0.03),
             arrowprops=dict(arrowstyle='->', color='green', linewidth=2),
             fontsize=12, color='green', fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.3))

plt.xlabel('k (Number of Neighbors)', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('KNN Performance: Accuracy vs Number of Neighbors', fontsize=14)
plt.legend(loc='best', fontsize=11)
plt.grid(True, alpha=0.3)
plt.xlim(0, 31)
plt.ylim(0.5, 1.02)

# Add shaded region showing overfitting vs underfitting
plt.axvspan(1, 5, alpha=0.1, color='blue', label='Potential Overfitting')
plt.axvspan(20, 30, alpha=0.1, color='red', label='Potential Underfitting')

plt.tight_layout()
plt.show()
```


```{python}
#| label: Different-decision-boundaries
#| echo: true


def plot_decision_boundary(X_train, y_train, k, ax, title):
    """Plot decision boundary for KNN with given k."""
    # Create mesh
    h = 0.1  # step size in mesh
    x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5
    y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    
    # Predict on mesh points
    mesh_points = np.c_[xx.ravel(), yy.ravel()]
    Z = knn_predict(X_train, y_train, mesh_points, k)
    Z = Z.reshape(xx.shape)
    
    # Plot decision boundary
    ax.contourf(xx, yy, Z, alpha=0.4, cmap=ListedColormap(['lightblue', 'lightcoral']))
    
    # Plot training points
    scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, 
                         cmap=ListedColormap(['blue', 'red']), 
                         edgecolor='black', s=50, alpha=0.8)
    
    ax.set_title(title, fontsize=12)
    ax.set_xlabel('x₁')
    ax.set_ylabel('x₂')
    ax.grid(True, alpha=0.3)
    
    return scatter

# Visualize decision boundaries for different k values
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

k_values_to_plot = [1, 3, 5, 10, 20, optimal_k]
titles = [f'k = {k}' if k != optimal_k else f'k = {k} (Optimal)' 
          for k in k_values_to_plot]

for idx, (k, title) in enumerate(zip(k_values_to_plot, titles)):
    plot_decision_boundary(x, y, k, axes[idx], title)

plt.suptitle('KNN Decision Boundaries for Different k Values', fontsize=16)
plt.tight_layout()
plt.show()
```

```{python}
#| label: Analyze-the-behavior-of-accuracy-with-k
#| echo: true


print("\n### Analysis of Results ###")

# Find k values with top 5 accuracies
top_k_indices = np.argsort(accuracies)[-5:][::-1]
print("\nTop 5 k values by test accuracy:")
for i, idx in enumerate(top_k_indices):
    print(f"{i+1}. k={k_values[idx]:2d}: Accuracy = {accuracies[idx]:.4f}")

# Analyze stability of accuracy around optimal k
k_window = 2
optimal_idx = optimal_k - 1  # Convert to 0-based index
window_start = max(0, optimal_idx - k_window)
window_end = min(len(accuracies), optimal_idx + k_window + 1)
window_accuracies = accuracies[window_start:window_end]
stability = np.std(window_accuracies)

print(f"\nStability analysis around optimal k={optimal_k}:")
print(f"Accuracy std in window [{optimal_k-k_window}, {optimal_k+k_window}]: {stability:.4f}")
print(f"Average accuracy in window: {np.mean(window_accuracies):.4f}")

# Bias-variance tradeoff discussion
print("\n### Bias-Variance Tradeoff ###")
print(f"k=1  accuracy: {accuracies[0]:.4f} (Low bias, high variance)")
print(f"k={optimal_k} accuracy: {optimal_accuracy:.4f} (Optimal tradeoff)")
print(f"k=30 accuracy: {accuracies[-1]:.4f} (High bias, low variance)")

print("\n### Summary ###")
print(f"1. Successfully implemented KNN algorithm from scratch")
print(f"2. Implementation verified against sklearn (>{(np.sum(y_pred_manual == y_pred_sklearn)/len(y_test)*100):.0f}% agreement)")
print(f"3. Optimal k value: {optimal_k} with test accuracy of {optimal_accuracy:.4f}")
print(f"4. The non-linear decision boundary is well-captured by KNN with appropriate k")
print(f"\nKey observations:")
print(f"- Small k values (1-3) show signs of overfitting with jagged decision boundaries")
print(f"- Large k values (>20) show signs of underfitting with overly smooth boundaries")
print(f"- The optimal k={optimal_k} provides a good balance between bias and variance")
print(f"- The wiggly sin-based boundary is effectively learned by the non-parametric KNN approach")
```