{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Machine Learning\"\n",
        "author: \"Vidhi Vashishth\"\n",
        "date: today\n",
        "---\n",
        "\n",
        "## 1b. Latent-Class MNL\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this analysis, I implement a latent-class multinomial logit (MNL) model on yogurt purchase data. The latent-class approach allows us to identify distinct consumer segments with different preferences, rather than assuming all consumers have identical preferences as in the standard MNL model.\n",
        "\n",
        "### Data Loading and Exploration"
      ],
      "id": "6497cef4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.optimize import minimize\n",
        "from scipy.special import logsumexp\n",
        "from scipy.stats import multivariate_normal\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the yogurt dataset\n",
        "yogurt_data = pd.read_csv('yogurt_data.csv')\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Dataset shape:\", yogurt_data.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(yogurt_data.head())\n",
        "print(\"\\nColumn names:\")\n",
        "print(yogurt_data.columns.tolist())\n",
        "\n",
        "# Analyze purchase patterns\n",
        "purchase_counts = (yogurt_data[['y1', 'y2', 'y3', 'y4']].sum())\n",
        "print(\"\\nPurchase counts by product:\")\n",
        "print(purchase_counts)\n",
        "\n",
        "# Visualize purchase distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "purchase_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Yogurt Purchases by Product', fontsize=14)\n",
        "plt.xlabel('Product')\n",
        "plt.ylabel('Number of Purchases')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e08820a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_The data provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were \"featured\" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current \"wide\" format into a \"long\" format._\n",
        "\n",
        "### Data Reshaping from Wide to long format"
      ],
      "id": "cb99b756"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def reshape_yogurt_data(df):\n",
        "    \"\"\"\n",
        "    Reshape yogurt data from wide to long format.\n",
        "    Each row becomes one alternative for one choice occasion.\n",
        "    \"\"\"\n",
        "    long_data = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        consumer_id = row['id']\n",
        "        \n",
        "        # For each alternative (yogurt 1-4)\n",
        "        for j in range(1, 5):\n",
        "            choice = row[f'y{j}']\n",
        "            feature = row[f'f{j}']\n",
        "            price = row[f'p{j}']\n",
        "            \n",
        "            long_data.append({\n",
        "                'id': consumer_id,\n",
        "                'alternative': j,\n",
        "                'choice': choice,\n",
        "                'feature': feature,\n",
        "                'price': price,\n",
        "                'choice_id': idx  # to track choice occasions\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(long_data)\n",
        "\n",
        "# Reshape the data\n",
        "long_data = reshape_yogurt_data(yogurt_data)\n",
        "print(\"\\nReshaped data shape:\", long_data.shape)\n",
        "print(\"\\nFirst 8 rows (2 choice occasions):\")\n",
        "print(long_data.head(8))\n",
        "\n",
        "# Verify reshaping\n",
        "n_choices = yogurt_data.shape[0]\n",
        "n_alternatives = 4\n",
        "print(f\"\\nOriginal data: {n_choices} choice occasions\")\n",
        "print(f\"Long format: {long_data.shape[0]} rows ({n_choices} × {n_alternatives} alternatives)\")"
      ],
      "id": "efd3d139",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_todo: Fit the standard MNL model on these data.  Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes._\n",
        "\n",
        "### Standard MNL model implementation"
      ],
      "id": "f32e5cba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def mnl_log_likelihood(params, X, y, n_alts=4):\n",
        "    \"\"\"\n",
        "    Calculate log-likelihood for standard MNL model.\n",
        "    \n",
        "    Parameters:\n",
        "    - params: array of parameters [price_coef, feature_coef]\n",
        "    - X: feature matrix (n_obs × n_features)\n",
        "    - y: choice indicators (n_obs,)\n",
        "    - n_alts: number of alternatives per choice occasion\n",
        "    \"\"\"\n",
        "    beta_price = params[0]\n",
        "    beta_feature = params[1]\n",
        "    \n",
        "    n_obs = len(y)\n",
        "    n_choices = n_obs // n_alts\n",
        "    \n",
        "    ll = 0\n",
        "    for i in range(n_choices):\n",
        "        start_idx = i * n_alts\n",
        "        end_idx = (i + 1) * n_alts\n",
        "        \n",
        "        # Calculate utilities for all alternatives in this choice set\n",
        "        utilities = (beta_price * X[start_idx:end_idx, 0] + \n",
        "                    beta_feature * X[start_idx:end_idx, 1])\n",
        "        \n",
        "        # Find which alternative was chosen\n",
        "        choice_idx = np.where(y[start_idx:end_idx] == 1)[0][0]\n",
        "        \n",
        "        # Log-likelihood contribution\n",
        "        ll += utilities[choice_idx] - logsumexp(utilities)\n",
        "    \n",
        "    return -ll  # Return negative for minimization\n",
        "\n",
        "# Prepare data for MNL estimation\n",
        "X = long_data[['price', 'feature']].values\n",
        "y = long_data['choice'].values\n",
        "\n",
        "# Estimate standard MNL\n",
        "init_params = [0, 0]  # Initial values for price and feature coefficients\n",
        "result_mnl = minimize(mnl_log_likelihood, init_params, args=(X, y), \n",
        "                     method='BFGS', options={'disp': False})\n",
        "\n",
        "print(\"\\n### Standard MNL Results ###\")\n",
        "print(f\"Price coefficient: {result_mnl.x[0]:.4f}\")\n",
        "print(f\"Feature coefficient: {result_mnl.x[1]:.4f}\")\n",
        "print(f\"Log-likelihood: {-result_mnl.fun:.4f}\")"
      ],
      "id": "549308ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Latent-class MNL implementation"
      ],
      "id": "50fcde21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class LatentClassMNL:\n",
        "    \"\"\"\n",
        "    Latent-class multinomial logit model using EM algorithm.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_classes, n_alts=4, max_iter=100, tol=1e-6):\n",
        "        self.n_classes = n_classes\n",
        "        self.n_alts = n_alts\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.fitted = False\n",
        "        \n",
        "    def initialize_params(self, n_features):\n",
        "        \"\"\"Initialize parameters randomly.\"\"\"\n",
        "        # Class-specific choice parameters\n",
        "        self.betas = np.random.randn(self.n_classes, n_features)\n",
        "        \n",
        "        # Class membership probabilities (equal initially)\n",
        "        self.class_probs = np.ones(self.n_classes) / self.n_classes\n",
        "        \n",
        "    def e_step(self, X, y, n_choices):\n",
        "        \"\"\"\n",
        "        E-step: Calculate posterior class membership probabilities.\n",
        "        \"\"\"\n",
        "        posteriors = np.zeros((n_choices, self.n_classes))\n",
        "        \n",
        "        for c in range(self.n_classes):\n",
        "            # Calculate likelihood for each choice under class c\n",
        "            for i in range(n_choices):\n",
        "                start_idx = i * self.n_alts\n",
        "                end_idx = (i + 1) * self.n_alts\n",
        "                \n",
        "                # Utilities for class c\n",
        "                utilities = X[start_idx:end_idx] @ self.betas[c]\n",
        "                \n",
        "                # Find chosen alternative\n",
        "                choice_idx = np.where(y[start_idx:end_idx] == 1)[0][0]\n",
        "                \n",
        "                # Choice probability\n",
        "                prob = np.exp(utilities[choice_idx] - logsumexp(utilities))\n",
        "                posteriors[i, c] = prob * self.class_probs[c]\n",
        "        \n",
        "        # Normalize posteriors\n",
        "        posteriors = posteriors / posteriors.sum(axis=1, keepdims=True)\n",
        "        return posteriors\n",
        "    \n",
        "    def m_step(self, X, y, posteriors, n_choices):\n",
        "        \"\"\"\n",
        "        M-step: Update parameters given posterior probabilities.\n",
        "        \"\"\"\n",
        "        # Update class probabilities\n",
        "        self.class_probs = posteriors.mean(axis=0)\n",
        "        \n",
        "        # Update class-specific parameters\n",
        "        for c in range(self.n_classes):\n",
        "            # Weighted MNL estimation for class c\n",
        "            def weighted_ll(params):\n",
        "                ll = 0\n",
        "                for i in range(n_choices):\n",
        "                    start_idx = i * self.n_alts\n",
        "                    end_idx = (i + 1) * self.n_alts\n",
        "                    \n",
        "                    utilities = X[start_idx:end_idx] @ params\n",
        "                    choice_idx = np.where(y[start_idx:end_idx] == 1)[0][0]\n",
        "                    \n",
        "                    ll += posteriors[i, c] * (utilities[choice_idx] - logsumexp(utilities))\n",
        "                return -ll\n",
        "            \n",
        "            result = minimize(weighted_ll, self.betas[c], method='BFGS', \n",
        "                            options={'disp': False})\n",
        "            self.betas[c] = result.x\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit latent-class MNL using EM algorithm.\n",
        "        \"\"\"\n",
        "        n_obs = len(y)\n",
        "        n_choices = n_obs // self.n_alts\n",
        "        n_features = X.shape[1]\n",
        "        \n",
        "        # Initialize parameters\n",
        "        self.initialize_params(n_features)\n",
        "        \n",
        "        # EM algorithm\n",
        "        prev_ll = -np.inf\n",
        "        for iteration in range(self.max_iter):\n",
        "            # E-step\n",
        "            posteriors = self.e_step(X, y, n_choices)\n",
        "            \n",
        "            # M-step\n",
        "            self.m_step(X, y, posteriors, n_choices)\n",
        "            \n",
        "            # Calculate log-likelihood\n",
        "            ll = self.log_likelihood(X, y, n_choices)\n",
        "            \n",
        "            # Check convergence\n",
        "            if abs(ll - prev_ll) < self.tol:\n",
        "                print(f\"Converged after {iteration + 1} iterations\")\n",
        "                break\n",
        "            prev_ll = ll\n",
        "        \n",
        "        self.fitted = True\n",
        "        self.final_ll = ll\n",
        "        self.posteriors = posteriors\n",
        "        \n",
        "    def log_likelihood(self, X, y, n_choices):\n",
        "        \"\"\"Calculate complete data log-likelihood.\"\"\"\n",
        "        ll = 0\n",
        "        for i in range(n_choices):\n",
        "            start_idx = i * self.n_alts\n",
        "            end_idx = (i + 1) * self.n_alts\n",
        "            \n",
        "            # Likelihood for each class\n",
        "            class_liks = np.zeros(self.n_classes)\n",
        "            for c in range(self.n_classes):\n",
        "                utilities = X[start_idx:end_idx] @ self.betas[c]\n",
        "                choice_idx = np.where(y[start_idx:end_idx] == 1)[0][0]\n",
        "                class_liks[c] = np.exp(utilities[choice_idx] - logsumexp(utilities))\n",
        "            \n",
        "            # Weighted by class probabilities\n",
        "            ll += np.log(np.sum(class_liks * self.class_probs))\n",
        "        \n",
        "        return ll\n",
        "    \n",
        "    def get_n_parameters(self):\n",
        "        \"\"\"Calculate total number of parameters.\"\"\"\n",
        "        n_features = self.betas.shape[1]\n",
        "        # Class-specific betas + class probabilities (minus 1 for identification)\n",
        "        return self.n_classes * n_features + (self.n_classes - 1)"
      ],
      "id": "be911121",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimating latent class models"
      ],
      "id": "bcd6e4ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Estimate latent-class models for different numbers of classes\n",
        "results = {}\n",
        "n_choices = len(yogurt_data)\n",
        "\n",
        "print(\"\\n### Estimating Latent-Class Models ###\")\n",
        "for n_classes in [2, 3, 4, 5]:\n",
        "    print(f\"\\nFitting {n_classes}-class model...\")\n",
        "    \n",
        "    # Fit model\n",
        "    model = LatentClassMNL(n_classes=n_classes)\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    # Calculate BIC\n",
        "    ll = model.final_ll\n",
        "    k = model.get_n_parameters()\n",
        "    bic = -2 * ll + k * np.log(n_choices)\n",
        "    \n",
        "    results[n_classes] = {\n",
        "        'model': model,\n",
        "        'log_likelihood': ll,\n",
        "        'n_parameters': k,\n",
        "        'bic': bic,\n",
        "        'class_probs': model.class_probs,\n",
        "        'betas': model.betas\n",
        "    }\n",
        "    \n",
        "    print(f\"  Log-likelihood: {ll:.2f}\")\n",
        "    print(f\"  Parameters: {k}\")\n",
        "    print(f\"  BIC: {bic:.2f}\")\n",
        "    print(f\"  Class sizes: {model.class_probs.round(3)}\")"
      ],
      "id": "2d6f6254",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_todo: How many classes are suggested by the $BIC = -2*\\ell_n  + k*log(n)$? (where $\\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) The Bayesian-Schwarz Information Criterion [link](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate -- akin to the adjusted R-squared for the linear regression model. Note, that a **lower** BIC indicates a better model fit, accounting for the number of parameters in the model._\n",
        "\n",
        "### Model selection using BIC"
      ],
      "id": "1532dba6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create BIC comparison plot\n",
        "n_classes_list = list(results.keys())\n",
        "bic_values = [results[n]['bic'] for n in n_classes_list]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_classes_list, bic_values, marker='o', markersize=10, \n",
        "         linewidth=2, color='darkblue')\n",
        "plt.xlabel('Number of Classes', fontsize=12)\n",
        "plt.ylabel('BIC', fontsize=12)\n",
        "plt.title('Model Selection: BIC vs Number of Latent Classes', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Highlight minimum BIC\n",
        "min_bic_idx = np.argmin(bic_values)\n",
        "optimal_classes = n_classes_list[min_bic_idx]\n",
        "plt.plot(optimal_classes, bic_values[min_bic_idx], 'ro', markersize=15)\n",
        "plt.annotate(f'Optimal: {optimal_classes} classes', \n",
        "             xy=(optimal_classes, bic_values[min_bic_idx]),\n",
        "             xytext=(optimal_classes + 0.3, bic_values[min_bic_idx] + 20),\n",
        "             arrowprops=dict(arrowstyle='->', color='red'),\n",
        "             fontsize=12, color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n### Model Selection Result ###\")\n",
        "print(f\"Optimal number of classes according to BIC: {optimal_classes}\")\n",
        "print(f\"BIC value: {results[optimal_classes]['bic']:.2f}\")"
      ],
      "id": "86c1a214",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameter comparison"
      ],
      "id": "3ebdd177"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare parameters between aggregate MNL and optimal latent-class model\n",
        "optimal_model = results[optimal_classes]\n",
        "\n",
        "# Create comparison visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Parameter values comparison\n",
        "param_names = ['Price', 'Feature']\n",
        "mnl_params = result_mnl.x\n",
        "\n",
        "# Prepare data for heatmap\n",
        "comparison_data = np.zeros((optimal_classes + 1, len(param_names)))\n",
        "comparison_data[0, :] = mnl_params\n",
        "\n",
        "for c in range(optimal_classes):\n",
        "    comparison_data[c + 1, :] = optimal_model['betas'][c]\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(comparison_data, \n",
        "            xticklabels=param_names,\n",
        "            yticklabels=['Aggregate MNL'] + [f'Class {i+1}' for i in range(optimal_classes)],\n",
        "            annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n",
        "            cbar_kws={'label': 'Parameter Value'},\n",
        "            ax=ax1)\n",
        "ax1.set_title('Parameter Estimates Comparison', fontsize=14)\n",
        "\n",
        "# Plot 2: Class sizes (for latent-class model)\n",
        "class_labels = [f'Class {i+1}' for i in range(optimal_classes)]\n",
        "class_sizes = optimal_model['class_probs']\n",
        "\n",
        "ax2.pie(class_sizes, labels=class_labels, autopct='%1.1f%%', \n",
        "        colors=plt.cm.Set3(range(optimal_classes)))\n",
        "ax2.set_title(f'Latent Class Sizes ({optimal_classes}-Class Model)', fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed comparison\n",
        "print(\"\\n### Parameter Comparison ###\")\n",
        "print(\"\\nAggregate MNL:\")\n",
        "print(f\"  Price coefficient: {mnl_params[0]:.4f}\")\n",
        "print(f\"  Feature coefficient: {mnl_params[1]:.4f}\")\n",
        "\n",
        "print(f\"\\n{optimal_classes}-Class Latent Model:\")\n",
        "for c in range(optimal_classes):\n",
        "    print(f\"\\nClass {c+1} (size: {optimal_model['class_probs'][c]:.1%}):\")\n",
        "    print(f\"  Price coefficient: {optimal_model['betas'][c][0]:.4f}\")\n",
        "    print(f\"  Feature coefficient: {optimal_model['betas'][c][1]:.4f}\")"
      ],
      "id": "a31df5de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation and Insights"
      ],
      "id": "04301339"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate and visualize heterogeneity in price sensitivity\n",
        "price_sensitivities = optimal_model['betas'][:, 0]\n",
        "feature_effects = optimal_model['betas'][:, 1]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Price sensitivity distribution\n",
        "ax1.bar(range(1, optimal_classes + 1), -price_sensitivities, \n",
        "        color='coral', edgecolor='black', alpha=0.7)\n",
        "ax1.axhline(y=-mnl_params[0], color='darkblue', linestyle='--', \n",
        "           label='Aggregate MNL', linewidth=2)\n",
        "ax1.set_xlabel('Class')\n",
        "ax1.set_ylabel('Price Sensitivity (absolute value)')\n",
        "ax1.set_title('Price Sensitivity by Class', fontsize=14)\n",
        "ax1.set_xticks(range(1, optimal_classes + 1))\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Feature effect distribution\n",
        "ax2.bar(range(1, optimal_classes + 1), feature_effects, \n",
        "        color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "ax2.axhline(y=mnl_params[1], color='darkblue', linestyle='--', \n",
        "           label='Aggregate MNL', linewidth=2)\n",
        "ax2.set_xlabel('Class')\n",
        "ax2.set_ylabel('Feature Effect')\n",
        "ax2.set_title('Feature/Promotion Effect by Class', fontsize=14)\n",
        "ax2.set_xticks(range(1, optimal_classes + 1))\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "6c5d8645",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary and Conclusions"
      ],
      "id": "b69db47f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n### Summary ###\")\n",
        "print(f\"1. The aggregate MNL model assumes all consumers have identical preferences.\")\n",
        "print(f\"2. The BIC criterion suggests {optimal_classes} latent classes provide the best model fit.\")\n",
        "print(f\"3. The latent-class model reveals significant heterogeneity in consumer preferences:\")\n",
        "\n",
        "# Identify key differences between classes\n",
        "for c in range(optimal_classes):\n",
        "    price_sens = -optimal_model['betas'][c][0]\n",
        "    feature_eff = optimal_model['betas'][c][1]\n",
        "    class_size = optimal_model['class_probs'][c]\n",
        "    \n",
        "    print(f\"\\n   Class {c+1} ({class_size:.1%} of consumers):\")\n",
        "    if price_sens > -mnl_params[0] * 1.2:\n",
        "        print(f\"   - High price sensitivity\")\n",
        "    elif price_sens < -mnl_params[0] * 0.8:\n",
        "        print(f\"   - Low price sensitivity\")\n",
        "    else:\n",
        "        print(f\"   - Moderate price sensitivity\")\n",
        "    \n",
        "    if feature_eff > mnl_params[1] * 1.2:\n",
        "        print(f\"   - Strongly influenced by features/promotions\")\n",
        "    elif feature_eff < mnl_params[1] * 0.8:\n",
        "        print(f\"   - Weakly influenced by features/promotions\")\n",
        "    else:\n",
        "        print(f\"   - Moderately influenced by features/promotions\")"
      ],
      "id": "48aaad52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2a. K Nearest Neighbors\n",
        "\n",
        "### Introduction\n",
        "In this section, I implement the K-Nearest Neighbors (KNN) algorithm from scratch and evaluate its performance on a synthetic binary classification dataset with a non-linear decision boundary. KNN is a simple yet powerful non-parametric algorithm that makes predictions based on the majority class of the k nearest neighbors in the feature space.\n",
        "\n",
        "### Generate Synthetic Dataset"
      ],
      "id": "4d0ac8b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate training data\n",
        "n = 100\n",
        "x1 = np.random.uniform(-3, 3, n)\n",
        "x2 = np.random.uniform(-3, 3, n)\n",
        "x = np.column_stack((x1, x2))\n",
        "\n",
        "# Define a wiggly boundary using sin function\n",
        "boundary = np.sin(4*x1) + x1\n",
        "y = (x2 > boundary).astype(int)\n",
        "\n",
        "# Create training dataframe\n",
        "train_data = pd.DataFrame({\n",
        "    'x1': x1,\n",
        "    'x2': x2,\n",
        "    'y': y\n",
        "})\n",
        "\n",
        "print(\"Training dataset shape:\", train_data.shape)\n",
        "print(\"\\nClass distribution in training set:\")\n",
        "print(train_data['y'].value_counts())\n",
        "print(f\"Class balance: {(y==1).sum()/len(y):.1%} positive class\")"
      ],
      "id": "caf3c43e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualise training data with decision boundary"
      ],
      "id": "8ee26855"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create figure with the wiggly boundary\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create a dense grid for plotting the true boundary\n",
        "x1_boundary = np.linspace(-3, 3, 1000)\n",
        "boundary_line = np.sin(4*x1_boundary) + x1_boundary\n",
        "\n",
        "# Plot the true decision boundary\n",
        "plt.plot(x1_boundary, boundary_line, 'k--', linewidth=2, \n",
        "         label='True Boundary: y = sin(4x₁) + x₁', alpha=0.7)\n",
        "\n",
        "# Plot the data points\n",
        "colors = ['blue', 'red']\n",
        "labels = ['Class 0 (y < boundary)', 'Class 1 (y > boundary)']\n",
        "\n",
        "for class_val in [0, 1]:\n",
        "    mask = y == class_val\n",
        "    plt.scatter(x1[mask], x2[mask], c=colors[class_val], \n",
        "                label=labels[class_val], alpha=0.6, edgecolor='black', s=50)\n",
        "\n",
        "plt.xlabel('x₁', fontsize=12)\n",
        "plt.ylabel('x₂', fontsize=12)\n",
        "plt.title('Training Data with Non-linear Decision Boundary', fontsize=14)\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(-3.2, 3.2)\n",
        "plt.ylim(-3.2, 3.2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "965991d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate test dataset"
      ],
      "id": "95d7cac4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate test data with different seed\n",
        "np.random.seed(123)  # Different seed for test data\n",
        "\n",
        "n_test = 100\n",
        "x1_test = np.random.uniform(-3, 3, n_test)\n",
        "x2_test = np.random.uniform(-3, 3, n_test)\n",
        "x_test = np.column_stack((x1_test, x2_test))\n",
        "\n",
        "# Apply same boundary rule\n",
        "boundary_test = np.sin(4*x1_test) + x1_test\n",
        "y_test = (x2_test > boundary_test).astype(int)\n",
        "\n",
        "# Create test dataframe\n",
        "test_data = pd.DataFrame({\n",
        "    'x1': x1_test,\n",
        "    'x2': x2_test,\n",
        "    'y': y_test\n",
        "})\n",
        "\n",
        "print(\"\\nTest dataset shape:\", test_data.shape)\n",
        "print(\"\\nClass distribution in test set:\")\n",
        "print(test_data['y'].value_counts())\n",
        "print(f\"Class balance: {(y_test==1).sum()/len(y_test):.1%} positive class\")"
      ],
      "id": "8889012c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implement KNN algorithm from scratch"
      ],
      "id": "74b4ab77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def euclidean_distance(x1, x2):\n",
        "    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n",
        "    return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "def knn_predict_single(X_train, y_train, x_test, k):\n",
        "    \"\"\"\n",
        "    Predict class for a single test point using KNN.\n",
        "    \n",
        "    Parameters:\n",
        "    - X_train: Training features (n_samples × n_features)\n",
        "    - y_train: Training labels (n_samples,)\n",
        "    - x_test: Single test point (n_features,)\n",
        "    - k: Number of neighbors\n",
        "    \n",
        "    Returns:\n",
        "    - Predicted class (0 or 1)\n",
        "    \"\"\"\n",
        "    # Calculate distances to all training points\n",
        "    distances = []\n",
        "    for i in range(len(X_train)):\n",
        "        dist = euclidean_distance(X_train[i], x_test)\n",
        "        distances.append((dist, y_train[i]))\n",
        "    \n",
        "    # Sort by distance and get k nearest neighbors\n",
        "    distances.sort(key=lambda x: x[0])\n",
        "    k_nearest = distances[:k]\n",
        "    \n",
        "    # Get labels of k nearest neighbors\n",
        "    k_labels = [label for _, label in k_nearest]\n",
        "    \n",
        "    # Return majority vote (with tie-breaking favoring class 1)\n",
        "    return 1 if sum(k_labels) >= k/2 else 0\n",
        "\n",
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "    \"\"\"\n",
        "    Predict classes for multiple test points using KNN.\n",
        "    \n",
        "    Parameters:\n",
        "    - X_train: Training features (n_samples × n_features)\n",
        "    - y_train: Training labels (n_samples,)\n",
        "    - X_test: Test features (n_test_samples × n_features)\n",
        "    - k: Number of neighbors\n",
        "    \n",
        "    Returns:\n",
        "    - Array of predictions\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    for i in range(len(X_test)):\n",
        "        pred = knn_predict_single(X_train, y_train, X_test[i], k)\n",
        "        predictions.append(pred)\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Test the implementation with k=5\n",
        "k_test = 5\n",
        "start_time = time.time()\n",
        "y_pred_manual = knn_predict(x, y, x_test, k=k_test)\n",
        "manual_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nManual KNN implementation (k={k_test}):\")\n",
        "print(f\"Time taken: {manual_time:.4f} seconds\")\n",
        "print(f\"Predictions shape: {y_pred_manual.shape}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_manual):.4f}\")"
      ],
      "id": "2f6bd334",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verifying implementation with sklearn"
      ],
      "id": "b42657b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare with sklearn's implementation\n",
        "knn_sklearn = KNeighborsClassifier(n_neighbors=k_test)\n",
        "start_time = time.time()\n",
        "knn_sklearn.fit(x, y)\n",
        "y_pred_sklearn = knn_sklearn.predict(x_test)\n",
        "sklearn_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nSklearn KNN implementation (k={k_test}):\")\n",
        "print(f\"Time taken: {sklearn_time:.4f} seconds\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_sklearn):.4f}\")\n",
        "\n",
        "# Check if predictions match\n",
        "matches = np.sum(y_pred_manual == y_pred_sklearn)\n",
        "print(f\"\\nPrediction agreement: {matches}/{len(y_test)} ({matches/len(y_test)*100:.1f}%)\")\n",
        "\n",
        "# Visualize predictions comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Manual implementation predictions\n",
        "ax1.scatter(x_test[y_pred_manual==0, 0], x_test[y_pred_manual==0, 1], \n",
        "           c='blue', label='Predicted 0', alpha=0.6, edgecolor='black')\n",
        "ax1.scatter(x_test[y_pred_manual==1, 0], x_test[y_pred_manual==1, 1], \n",
        "           c='red', label='Predicted 1', alpha=0.6, edgecolor='black')\n",
        "ax1.set_title(f'Manual KNN Predictions (k={k_test})', fontsize=14)\n",
        "ax1.set_xlabel('x₁')\n",
        "ax1.set_ylabel('x₂')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Sklearn predictions\n",
        "ax2.scatter(x_test[y_pred_sklearn==0, 0], x_test[y_pred_sklearn==0, 1], \n",
        "           c='blue', label='Predicted 0', alpha=0.6, edgecolor='black')\n",
        "ax2.scatter(x_test[y_pred_sklearn==1, 0], x_test[y_pred_sklearn==1, 1], \n",
        "           c='red', label='Predicted 1', alpha=0.6, edgecolor='black')\n",
        "ax2.set_title(f'Sklearn KNN Predictions (k={k_test})', fontsize=14)\n",
        "ax2.set_xlabel('x₁')\n",
        "ax2.set_ylabel('x₂')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "e0be099c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance evaluation"
      ],
      "id": "acfc9ebf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test for k values from 1 to 30\n",
        "k_values = range(1, 31)\n",
        "accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "print(\"\\nEvaluating KNN for different k values...\")\n",
        "for k in k_values:\n",
        "    # Test set accuracy\n",
        "    y_pred = knn_predict(x, y, x_test, k)\n",
        "    test_acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(test_acc)\n",
        "    \n",
        "    # Training set accuracy (for comparison)\n",
        "    y_pred_train = knn_predict(x, y, x, k)\n",
        "    train_acc = accuracy_score(y, y_pred_train)\n",
        "    train_accuracies.append(train_acc)\n",
        "    \n",
        "    if k % 5 == 0:\n",
        "        print(f\"k={k:2d}: Test Accuracy = {test_acc:.4f}, Train Accuracy = {train_acc:.4f}\")\n",
        "\n",
        "# Find optimal k\n",
        "optimal_k = k_values[np.argmax(accuracies)]\n",
        "optimal_accuracy = max(accuracies)\n",
        "\n",
        "print(f\"\\nOptimal k value: {optimal_k}\")\n",
        "print(f\"Maximum test accuracy: {optimal_accuracy:.4f}\")"
      ],
      "id": "549ace52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy by choice of k"
      ],
      "id": "537476a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create accuracy plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# Plot both training and test accuracies\n",
        "plt.plot(k_values, train_accuracies, 'b-', linewidth=2, \n",
        "         label='Training Accuracy', marker='o', markersize=4, alpha=0.7)\n",
        "plt.plot(k_values, accuracies, 'r-', linewidth=2, \n",
        "         label='Test Accuracy', marker='s', markersize=4, alpha=0.7)\n",
        "\n",
        "# Highlight optimal k\n",
        "plt.scatter(optimal_k, optimal_accuracy, color='green', s=200, \n",
        "            zorder=5, edgecolor='black', linewidth=2)\n",
        "plt.annotate(f'Optimal k={optimal_k}\\nAccuracy={optimal_accuracy:.3f}', \n",
        "             xy=(optimal_k, optimal_accuracy),\n",
        "             xytext=(optimal_k + 2, optimal_accuracy - 0.03),\n",
        "             arrowprops=dict(arrowstyle='->', color='green', linewidth=2),\n",
        "             fontsize=12, color='green', fontweight='bold',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.3))\n",
        "\n",
        "plt.xlabel('k (Number of Neighbors)', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('KNN Performance: Accuracy vs Number of Neighbors', fontsize=14)\n",
        "plt.legend(loc='best', fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlim(0, 31)\n",
        "plt.ylim(0.5, 1.02)\n",
        "\n",
        "# Add shaded region showing overfitting vs underfitting\n",
        "plt.axvspan(1, 5, alpha=0.1, color='blue', label='Potential Overfitting')\n",
        "plt.axvspan(20, 30, alpha=0.1, color='red', label='Potential Underfitting')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "23ea4d86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Different decision boundaries"
      ],
      "id": "2d2396f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_decision_boundary(X_train, y_train, k, ax, title):\n",
        "    \"\"\"Plot decision boundary for KNN with given k.\"\"\"\n",
        "    # Create mesh\n",
        "    h = 0.1  # step size in mesh\n",
        "    x_min, x_max = X_train[:, 0].min() - 0.5, X_train[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_train[:, 1].min() - 0.5, X_train[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Predict on mesh points\n",
        "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = knn_predict(X_train, y_train, mesh_points, k)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=ListedColormap(['lightblue', 'lightcoral']))\n",
        "    \n",
        "    # Plot training points\n",
        "    scatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
        "                         cmap=ListedColormap(['blue', 'red']), \n",
        "                         edgecolor='black', s=50, alpha=0.8)\n",
        "    \n",
        "    ax.set_title(title, fontsize=12)\n",
        "    ax.set_xlabel('x₁')\n",
        "    ax.set_ylabel('x₂')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    return scatter\n",
        "\n",
        "# Visualize decision boundaries for different k values\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "k_values_to_plot = [1, 3, 5, 10, 20, optimal_k]\n",
        "titles = [f'k = {k}' if k != optimal_k else f'k = {k} (Optimal)' \n",
        "          for k in k_values_to_plot]\n",
        "\n",
        "for idx, (k, title) in enumerate(zip(k_values_to_plot, titles)):\n",
        "    plot_decision_boundary(x, y, k, axes[idx], title)\n",
        "\n",
        "plt.suptitle('KNN Decision Boundaries for Different k Values', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "9a0049bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze the behavior of accuracy with k\n",
        "print(\"\\n### Analysis of Results ###\")\n",
        "\n",
        "# Find k values with top 5 accuracies\n",
        "top_k_indices = np.argsort(accuracies)[-5:][::-1]\n",
        "print(\"\\nTop 5 k values by test accuracy:\")\n",
        "for i, idx in enumerate(top_k_indices):\n",
        "    print(f\"{i+1}. k={k_values[idx]:2d}: Accuracy = {accuracies[idx]:.4f}\")\n",
        "\n",
        "# Analyze stability of accuracy around optimal k\n",
        "k_window = 2\n",
        "optimal_idx = optimal_k - 1  # Convert to 0-based index\n",
        "window_start = max(0, optimal_idx - k_window)\n",
        "window_end = min(len(accuracies), optimal_idx + k_window + 1)\n",
        "window_accuracies = accuracies[window_start:window_end]\n",
        "stability = np.std(window_accuracies)\n",
        "\n",
        "print(f\"\\nStability analysis around optimal k={optimal_k}:\")\n",
        "print(f\"Accuracy std in window [{optimal_k-k_window}, {optimal_k+k_window}]: {stability:.4f}\")\n",
        "print(f\"Average accuracy in window: {np.mean(window_accuracies):.4f}\")\n",
        "\n",
        "# Bias-variance tradeoff discussion\n",
        "print(\"\\n### Bias-Variance Tradeoff ###\")\n",
        "print(f\"k=1  accuracy: {accuracies[0]:.4f} (Low bias, high variance)\")\n",
        "print(f\"k={optimal_k} accuracy: {optimal_accuracy:.4f} (Optimal tradeoff)\")\n",
        "print(f\"k=30 accuracy: {accuracies[-1]:.4f} (High bias, low variance)\")\n",
        "\n",
        "print(\"\\n### Summary ###\")\n",
        "print(f\"1. Successfully implemented KNN algorithm from scratch\")\n",
        "print(f\"2. Implementation verified against sklearn (>{(np.sum(y_pred_manual == y_pred_sklearn)/len(y_test)*100):.0f}% agreement)\")\n",
        "print(f\"3. Optimal k value: {optimal_k} with test accuracy of {optimal_accuracy:.4f}\")\n",
        "print(f\"4. The non-linear decision boundary is well-captured by KNN with appropriate k\")\n",
        "print(f\"\\nKey observations:\")\n",
        "print(f\"- Small k values (1-3) show signs of overfitting with jagged decision boundaries\")\n",
        "print(f\"- Large k values (>20) show signs of underfitting with overly smooth boundaries\")\n",
        "print(f\"- The optimal k={optimal_k} provides a good balance between bias and variance\")\n",
        "print(f\"- The wiggly sin-based boundary is effectively learned by the non-parametric KNN approach\")"
      ],
      "id": "fb820ecf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/conda/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}